* A Complete And Easy Guide To Check Elasticsearch                  :BLOG:DB:
:PROPERTIES:
:type:     DevOps,Tool,DataBase,Elasticsearch
:END:
---------------------------------------------------------------------
To poll Elasticsearch db status, we usually need to learn and try many many REST API. When we're new, it would take quite a while to put all the pieces together.

As Ops/DevOps, we are usually more concerned about cluster health and data inside. Enclosed is *A Simple Step-By-Step Guide* to check elasticsearch. Cluster, Nodes, Shards, Indices, Documents And Beyond.

[[image-blog:The Complete Guide To Query And Check Elasticsearch][https://www.dennyzhang.com/wp-content/uploads/denny/query_elasticsearch.png]]
---------------------------------------------------------------------
Elasticsearch Cheat Sheet for impatient users:

| What To Check         | Summary                                        |
|-----------------------+------------------------------------------------|
| elasticsearch-head[1] | A web tool: [[http://localhost:9200/_plugin/head/][$es_ip:$es_port/_plugin/head/]]      |
| elasticdump           | Import and export tools                        |
|-----------------------+------------------------------------------------|
| Get cluster health    | $es_ip:$es_port/_cluster/health?pretty         |
| Get cluster setting   | $es_ip:$es_port/_cluster/settings?v            |
| Get Index Setting     | $es_ip:$es_port/$es_name/_settings?v           |
|-----------------------+------------------------------------------------|
| List Nodes            | $es_ip:$es_port/_cat/nodes?v                   |
| List shards           | $es_ip:$es_port/_cat/shards?v                  |
| List indices          | $es_ip:$es_port/_cat/indices?v                 |
| Get allocation        | $es_ip:$es_port/_cat/allocation?v              |
| Get Version           | $es_ip:$es_port                                |
| Version By CLI        | bin/elasticsearch --version                    |
|-----------------------+------------------------------------------------|
| Indice Summary        | $es_ip:$es_port/$index/_count?pretty           |
| Indice Stats          | $es_ip:$es_port/$index/_stats?pretty           |
|-----------------------+------------------------------------------------|
| Get all docs          | $es_ip:$es_port/$index/_search?pretty          |
| Full Text Search      | $es_ip:$es_port/$index/_search?q=50            |
| Search By field       | $es_ip:$es_port/$index/_search?q=f1:50         |
| Search By 2 fields    | $es_ip:$es_port/$index/_search?q="f1:v1&f2=v2" |
** 1. Basic Intro About Elasticsearch
Generally speaking, elasticsearch is an efficient and API friendly search engine based on Lucene.

Some key concepts to know, before we dive deeper.
- Cluster: a collection of ES nodes.
- Nodes: server/instance stores data or coordinate with indexing and search capabilities.
- _Index_: a collection of documents. Like tables in MySQL.
- Type: To ensure optimal performance, we can define mappings for data types.
- _Document_: a basic unit of information. Like record in SQL.
- Shards: It allows us to horizontally split a big index, and store in multiple nodes.
** 2. GUI Tool: elasticsearch-head
elasticsearch-head is a web front end for browsing and interacting with an Elastic Search cluster[1]. It is pretty much like phpMyAdmin for mysql[2]. By default, elasticsearch doesn't install this plugin.

Though its UI looks a bit primitive and less attracting, it's certainly a useful tool for dummies like me. I frequently use the "Structured Query" menu to grab some random records, in order to understand unfamiliar ES indices.

http://localhost:9200/_plugin/head

[[image-blog:Elasticsearch head plugin][https://www.dennyzhang.com/wp-content/uploads/denny/elasticsearch_head_overview.png]]
** 3. Elasticsearch Global Info By API
Time for *complicated, yet powerful REST API*.

- Check cluster health.
#+BEGIN_SRC sh
curl \
 http://localhost:9200/_cluster/health?pretty
#+END_SRC

[[image-blog:Elasticsearch cluster health][https://www.dennyzhang.com/wp-content/uploads/denny/es_cluster_health.png]]

Usually we pay attention to "status" field, which should be *green*. When it's *yellow*, probably we have unassigned shards or unavailable nodes. When it's *red*, all primary shards of certain indices are inactive. This is truely bad.

- Check global setting and cluster stats[3].
#+BEGIN_SRC sh
# Get version
curl http://localhost:9200

# Get global settings
curl \
 http://localhost:9200/_cluster/settings

# Poll cluster stats
curl http://localhost:9200/_stats?pretty
#+END_SRC

Get version by CLI, without starting ES service.
#+BEGIN_SRC sh
bin/elasticsearch --version
#+END_SRC

- List nodes and shards
#+BEGIN_SRC sh
# List nodes
curl http://localhost:9200/_cat/nodes?v

# List shards
curl http://localhost:9200/_cat/shards?v
#+END_SRC

- *Show elasticsearch slow query[4]*: As DevOps/Ops, we definitely want to be on top of this. To enable this, make sure elasticsearch.yml is proplery configured. Tips: If you have problems to verify your setting about this, temporarily change thresholds to zero. Then every fetch operation should generate a slow log.
** 4. Check Elasticsearch Index By API
- Show Basic Summary Of An Elasticsearch Index
#+BEGIN_SRC sh
curl \
http://localhost:9200/$index/_count?pretty
#+END_SRC

[[image-blog:Summary Info For An Elasticsearch Index][https://www.dennyzhang.com/wp-content/uploads/denny/es_index_summary.png]]

"count" indicates how many records in the index. "_shards" shows how the index is horizontally split.
- Check DB schema of An Index or A field.
#+BEGIN_SRC sh
# Get Index Mapping
curl \
localhost:9200/$index/_mapping?pretty

# Get Field Mapping
curl \
localhost:9200/$index/_mapping/$type
#+END_SRC

- Indices verbose statistics.[5].
#+BEGIN_SRC sh
curl \
http://localhost:9200/$index/_stats
#+END_SRC
** 5. Check Documents Inside indices
- *Get all documents of an index*. Be careful with big indices. You might get tons of output running this command.
#+BEGIN_SRC sh
curl \
http://localhost:9200/$index/_search?pretty
#+END_SRC

- Full Text Search
#+BEGIN_SRC sh
# Get documents contains 50
curl \
http://localhost:9200/$index/_search?q=50
#+END_SRC

- URI Search: filter by field.[6] URI search indicates parsing request parameters from URI.
#+BEGIN_SRC sh
# Get documents whose f1 is 50
curl \
http://localhost:9200/$index/_search?q=f1:50
#+END_SRC

- Get documents with paging mechanism.[7]
#+BEGIN_SRC sh
# Get iterator, fetching 2 records each time
curl \
"localhost:9200/$index/_search?scroll=1m&size=2"\
-d '{
    "query" : {
        "match_all" : {}
    }
}'

# Get scroll id from previous command
# Then keep polling records of next page
curl 'localhost:9200/_search/scroll'  \
-d' {
    "scroll" : "1m",
    "scroll_id" : "INPUT_YOUR_SCROLL_ID"
}'
#+END_SRC
- Advanced Search With Request Body.[8] This would fall into the realm of lucene now.
#+BEGIN_SRC sh
curl \
http://localhost:9200/$index/_search?pretty\
 -d '{
    "query": {
        "query_string":
        {"query": "(field1:value1)
            AND (field2:value2)"}
    }
}'
#+END_SRC
** 6. More Tools To Recommend
- elasticdump is a 3rd tools to import and export elasticsearch indices[9]. Since it's a npm module, we have to install nodejs/npm to use it. _Frankly speaking, this is not my preference_. And elasticsearch has built-in support, called snapshot[10].

  However I would still recommand elasticdump for small indices. Why? Snapshot a bit complicated than I would expect. And we have to configure elasticsearch.yml and restart es instances to take effect. Too intruding. Compared to elasticdump, I wish elasticsearch can have more native support in the future version.

Posts: [[https://www.dennyzhang.com/tag/Elasticsearch][Tag #Elasticsearch]]
[display-posts tag="Elasticsearch" posts_per_page="20"]

[1] https://mobz.github.io/elasticsearch-head/
[2] https://www.phpmyadmin.net
[3] www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html
[4] www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-slowlog.html
[5] www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html
[6] www.elastic.co/guide/en/elasticsearch/reference/current/search-uri-request.html
[7] www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html
[8] www.elastic.co/guide/en/elasticsearch/reference/current/search-request-body.html
[9] https://github.com/taskrabbit/elasticsearch-dump
[10] www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html
#+BEGIN_HTML
<a href="https://github.com/dennyzhang/www.dennyzhang.com/tree/master/posts/query_elasticsearch"><img align="right" width="200" height="183" src="https://www.dennyzhang.com/wp-content/uploads/denny/watermark/github.png" /></a>

<div id="the whole thing" style="overflow: hidden;">
<div style="float: left; padding: 5px"> <a href="https://www.linkedin.com/in/dennyzhang001"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/linkedin.png" alt="linkedin" /></a></div>
<div style="float: left; padding: 5px"><a href="https://github.com/dennyzhang"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/github.png" alt="github" /></a></div>
<div style="float: left; padding: 5px"><a href="https://www.dennyzhang.com/slack" target="_blank" rel="nofollow"><img src="https://slack.dennyzhang.com/badge.svg" alt="slack"/></a></div>
</div>

<br/><br/>
<a href="http://makeapullrequest.com" target="_blank" rel="nofollow"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" alt="PRs Welcome"/></a>
#+END_HTML

Blog URL: https://www.dennyzhang.com/query_elasticsearch
* misc                                                             :noexport:
** basic use
| Name                                                 | Summary                                                            |
|------------------------------------------------------+--------------------------------------------------------------------|
| /usr/share/elasticsearch/bin/elasticsearch --version | check version                                                      |
|------------------------------------------------------+--------------------------------------------------------------------|
| configuration files                                  | ls /etc/elasticsearch/elasticsearch.yml*                           |
| log files                                            | ls -lth /var/log/elasticsearch                                     |
|------------------------------------------------------+--------------------------------------------------------------------|
| port 9200                                            |                                                                    |
| port 9300 -- 9400                                    |                                                                    |
|------------------------------------------------------+--------------------------------------------------------------------|
| elasticsearch node status                            | curl -XGET localhost:9200/_nodes/?pretty                           |
| check shard allocation                               | curl http://localhost:9200/_cat/allocation?v                       |
|------------------------------------------------------+--------------------------------------------------------------------|
| view index                                           | curl http://localhost:9200/.kibana/                                |
| view data inside an index                            | curl http://localhost:9200/.kibana/_search?q=1 python -m json.tool |
| view data inside an index                            | curl http://localhost:9200/.kibana/_search?q=2                     |
|------------------------------------------------------+--------------------------------------------------------------------|
| es stats                                             | curl http://localhost:9200/_stats                                  |

curl http://localhost:9200/_stats | python -m json.tool

curl -XGET 'localhost:10013/logstash-2015.08.14/json/_search?pretty'
[8/14/15, 6:28:11 PM] Wyman Liao: you are right, use below command can get the message from elasticsearch:
curl -XGET 'localhost:10013/logstash-2015.08.14/json/_search?q=tenantId:tenantId9&pretty=true'
[8/14/15, 6:30:59 PM] Wyman Liao: below can get the log I just updated in that file:
curl -XGET 'localhost:10013/logstash-2015.08.14/json/_search?q=tenantId:tenantId3iwyman&pretty=true'
** curl -XGET http://aio:9200/_cluster/health?level=indices
#+BEGIN_EXAMPLE
root@aio:/var/log/elasticsearch# curl -XGET http://aio:9200/_cluster/health?level=indices
{"cluster_name":"mdm","status":"red","timed_out":false,"number_of_nodes":2,"number_of_data_nodes":1,"active_primary_shards":1,"active_shards":1,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":3,"delayed_unassigned_shards":0,"number_of_pending_tasks":0,"number_of_in_flight_fetch":0,"task_max_waiting_in_queue_millis":0,"active_shards_percent_as_number":25.0,

"indices":{
".marvel-es-2016.02.04":{"status":"red","number_of_shards":1,"number_of_replicas":1,"active_primary_shards":0,"active_shards":0,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":2},
".marvel-es-data":{"status":"yellow","number_of_shards":1,"number_of_replicas":1,"active_primary_shards":1,"active_shards":1,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":1}
}}
#+END_EXAMPLE
** check elasticsearch cluster
#+BEGIN_EXAMPLE
root@kitchen-autotest-6nodes-node3:/# curl http://172.17.0.6:9200/_cat/allocation?v
shards disk.used disk.avail disk.total disk.percent host                          ip         node
     5   526.2gb    376.1gb    902.4gb           58 kitchen-autotest-6nodes-node3 172.17.0.5 node3-ubuntu-1404
     0        0b                                    kitchen-autotest-6nodes-node6 172.17.0.8 kitchen-autotest-6nodes-node6
     6   526.2gb    376.1gb    902.4gb           58 kitchen-autotest-6nodes-node4 172.17.0.6 node4-ubuntu-1404
     0        0b                                    kitchen-autotest-6nodes-node5 172.17.0.7 kitchen-autotest-6nodes-node5
#+END_EXAMPLE
** network.bind_host/network.host
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-network.html
** transport client VS node client
http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_transport_client_versus_node_client.html
** multicast/unicast
** How is cluster? And what if multiple master node?
** multiple "master" nodes?
http://stackoverflow.com/questions/23188325/elasticsearch-can-i-use-multiple-master-nodes-why
Q: Can I use multiple nodes as cluster master?
A: You cannot have more than one master node.

Q: Why should I do this? Maybe for distributing queries?
A: Consider you have 3 nodes n1, n2 and n3 that all contain data, and currently n1 is selected as the master master node. If you query in n2 node the query will be distributed to all corresponding shards of indexes[replica shard or primary shard]. The result from each shards are combined and return back to you (see the query phase docs).

It's not necessary to distribute the query by master node. Any node data or master or non data node can act as router[Distributing search queries].

Q: can master node be a smallest machine than data nodes? My current cluster is:

n1 - 8gb ram, 4 cpu - (x) master - ( ) data
n2 - 4gb ram, 2 cpu - ( ) master - (x) data
n3 - 4gb ram, 2 cpu - ( ) master - (x) data
n4 - 4gb ram, 2 cpu - ( ) master - (x) data
n5 - 4gb ram, 2 cpu - ( ) master - (x) data
All my queries are sent to N1, and I see in HTOP that master node is always easily and fresh CPU/RAM usage and data nodes gets most of cpu/ram usage.

A: yes the master node can be small if the node does not contain data because it need not take care of data management.Its only work is to just route the queries to corresponding nodes and return the result to you. If the master node contains data then you should have configuration more than an data node. because it have 2 works [data management,routing query]..
** #  --8<-------------------------- separator ------------------------>8--
** TODO How does elasticsearch unicast discovery work?
http://stackoverflow.com/questions/16821101/how-to-set-up-es-cluster
** TODO How does elasticsearch auto discovery work?
http://stackoverflow.com/questions/16821101/how-to-set-up-es-cluster
** TODO [#A] elasticsearch avoid SPOF for master node?
** HALF elasticsearch reindex: Cut big elasticsearch shard of huge indice into small pieces
https://trello.com/c/hOOCjXMC
#+BEGIN_EXAMPLE
[3/16/15, 3:31:24 PM] Shivang: this is for everyone (and maybe more for denny :) ):
[3/16/15, 3:31:25 PM] Shivang: http://www.elastic.co/guide/en/elasticsearch/guide/current/reindex.html
[3/16/15, 3:31:54 PM] Shivang: this is under the assumption that sometimes we want to "update" some types/analyzers and/or increase the number of shards
[3/16/15, 3:32:05 PM] Shivang: we will need to reindex data .. and that's the fastest way of doing it .
#+END_EXAMPLE

#+BEGIN_EXAMPLE
# create index with 3 shards and 2 replicas
curl -XPUT 'http://localhost:9200/master-index-8cd6e43115e9416eb23609486fa053e3-1?pretty' -d '
{
"settings" : {
"index" : {
"number_of_shards" : 1,
"number_of_replicas" : 0
}
}
}'

# reindex index
curl -XPOST 'http://localhost:9200/_reindex?pretty' -d '
{
"conflicts": "proceed",
"source": {
"index": "master-index-8cd6e43115e9416eb23609486fa053e3"
},
"dest": {
"index": "master-index-8cd6e43115e9416eb23609486fa053e3-1",
"op_type": "create"
}
}'

# get all re-index tasks
curl -XGET 'http://localhost:9200/_tasks?detailed=true&actions=*reindex&pretty'

# add index to existing alias and remove old index from that alias
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
"actions": [
{ "remove": {
"alias": "master-8cd6e43115e9416eb23609486fa053e3",
"index": "master-index-8cd6e43115e9416eb23609486fa053e3"
}},
{ "add": {
"alias": "master-8cd6e43115e9416eb23609486fa053e3",
"index": "master-index-8cd6e43115e9416eb23609486fa053e3-1"
}}
]
}'

# delete index
curl -XDELETE 'http://localhost:9200/master-index-8cd6e43115e9416eb23609486fa053e3?pretty'
#+END_EXAMPLE
** TODO setup elasticsearch and elasticsearch cluster
** TODO elastic search for 1 Billion data
http://www.elasticsearch.org/videos/heavy-lifting-hipchat-scaled-1-billion-messages/
#+BEGIN_EXAMPLE
[3/8/15, 1:50:26 PM] Shivang: A very good presentation from Atlassian and how they use ES (and got rid of CouchDB altogether) : http://www.elasticsearch.org/videos/heavy-lifting-hipchat-scaled-1-billion-messages/
[3/8/15, 1:51:13 PM] Shivang: @denny: also talks a lot about devops with ES
[3/8/15, 1:51:19 PM] denny: nice
[3/8/15, 1:52:47 PM] denny: I'm specially interested in:
- SLA backup and monitor for data of different importance.
- How they handle data retention
[3/8/15, 1:53:46 PM] Shivang: they are talking about backups before v1.0 of ES
[3/8/15, 1:53:53 PM] Shivang: and they did it through amazon ebs snapshots
[3/8/15, 1:54:07 PM] Shivang: ES does have iterative backups now supported
[3/8/15, 1:54:17 PM] Shivang: i will find that and send over the links
[3/8/15, 1:54:40 PM] denny: Incremental backup is nice.

Yes, please keep posting like these.
[3/8/15, 1:54:51 PM] denny: If I have more input after some research, I will do the same here.
[3/8/15, 1:55:06 PM] Shivang: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-snapshots.html
[3/8/15, 1:56:40 PM] Shivang: http://www.elasticsearch.org/videos/big-data-search-and-analytics/
[3/8/15, 1:56:53 PM] denny: I'm doing some interesting experiment of ELK this weekend, also some openstack stuff.
[3/8/15, 1:56:53 PM] Shivang: that one is by the cto himself .. amazing presentation
#+END_EXAMPLE
** #  --8<-------------------------- separator ------------------------>8--
** DONE Install elasticsearch
   CLOSED: [2015-01-22 Thu 17:53]
http://www.elasticsearch.org/overview/elkdownloads/
https://www.elastic.co/guide/en/elasticsearch/guide/current/_installing_elasticsearch.html

Download and unzip the latest Elasticsearch distribution
wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.2.zip

2
cd elasticsearch-1.4.2
Run bin/elasticsearch on Unix,
or bin/elasticsearch.bat on Windows

3

Run curl -X GET http://localhost:9200/
** DONE [#A] elasticsearch add failover
   CLOSED: [2015-09-10 Thu 10:55]
https://www.elastic.co/guide/en/elasticsearch/guide/current/_add_failover.html

As long as the second node has the same cluster.name as the first node
(see the ./config/elasticsearch.yml file), it should automatically
discover and join the cluster run by the first node.
** DONE Elasticsearch comes with two built-in clients
   CLOSED: [2015-09-09 Wed 22:55]
https://www.elastic.co/guide/en/elasticsearch/guide/current/_talking_to_elasticsearch.html

Node client
The node client joins a local cluster as a non data node. In other words, it doesn't hold any data itself, but it knows what data lives on which node in the cluster, and can forward requests directly to the correct node.

Transport client
The lighter-weight transport client can be used to send requests to a remote cluster. It doesn't join the cluster itself, but simply forwards requests to a node in the cluster.
** DONE elasticsearch concept: primary shards and replica shard
   CLOSED: [2015-09-09 Wed 23:21]
https://www.elastic.co/guide/en/elasticsearch/reference/current/_basic_concepts.html
| Concept             | Summary                                                                             |
|---------------------+-------------------------------------------------------------------------------------|
| Near Realtime (NRT) |                                                                                     |
| Cluster             |                                                                                     |
| Node                |                                                                                     |
| Index               | a collection of documents that have somewhat similar characteristics.               |
| Type                | a logical category/partition of your index whose semantics is completely up to you. |
| Document            |                                                                                     |
| Shards & Replicas   |                                                                                     |

Index
An index is a collection of documents that have somewhat similar
characteristics. For example, you can have an index for customer data,
another index for a product catalog, and yet another index for order
data. An index is identified by a name (that must be all lowercase)
and this name is used to refer to the index when performing indexing,
search, update, and delete operations against the documents in it.

In a single cluster, you can define as many indexes as you want.

Type
Within an index, you can define one or more types. A type is a logical
category/partition of your index whose semantics is completely up to
you. In general, a type is defined for documents that have a set of
common fields. For example, let's assume you run a blogging platform
and store all your data in a single index. In this index, you may
define a type for user data, another type for blog data, and yet
another type for comments data.

https://www.elastic.co/guide/en/elasticsearch/guide/current/_add_an_index.html

Shards are how Elasticsearch distributes data around your cluster.

Think of shards as containers for data. Documents are stored in
shards, and shards are allocated to nodes in your cluster. As your
cluster grows or shrinks, Elasticsearch will automatically migrate
shards between nodes so that the cluster remains balanced.

- primary shard: Each document in your index belongs to a single primary
shard, so the number of primary shards that you have determines the
maximum amount of data that your index can hold.

- A replica shard is just a copy of a primary shard.
** DONE elasticsearch add a cluster
   CLOSED: [2015-09-10 Thu 12:02]
https://www.elastic.co/guide/en/elasticsearch/guide/current/distributed-cluster.html
** #  --8<-------------------------- separator ------------------------>8--
** DONE [#A] elasticsearch backup
   CLOSED: [2015-12-23 Wed 18:10]
curl -XPOST "http://833575d34be3:9200/indexname/typename/optionalUniqueId" -d "{ \"field\" : \"value\"}"

curl -XPUT http://833575d34be3:9200/_snapshot/my_backup -d '{ "type": "fs", "settings": { "location": "/data/backup"}}'

curl -XPUT http://833575d34be3:9200/_snapshot/my_backup/snapshot_1

ls -lth /data/backup

http://www.ttlsa.com/elk/elk-elasticsearch-snapshot-restore/
假如共享文件存储介质挂载在/mount/back目录下,需要在elasticsearch.yml添加如下配置:
path.repo: ["/mount/back"]

mkdir -p /root/my_backup

curl http://127.0.0.1:9200/_snapshot/my_backup -H "content-type: application/json" -d "{\"type\": \"fs\", \"settings\": {\"location\": \"/root/my_backup\", \"max_snapshot_bytes_per_sec\": \"50mb\", \"max_restore_bytes_per_sec\": \"50mb\"}}"

curl -XPUT http://127.0.0.1:9200/_snapshot/my_backup -d '{ "type": "fs", "settings": { "location": "/root/backup"}}'

https://www.elastic.co/guide/en/elasticsearch/guide/current/backing-up-your-cluster.html
POST _snapshot/my_backup/
{
    "type": "fs",
    "settings": {
        "location": "/mount/backups/my_backup",
        "max_snapshot_bytes_per_sec" : "50mb",
        "max_restore_bytes_per_sec" : "50mb"
    }
}
** DONE elasticsearch network.host, network.bind_host and network.publish_host
   CLOSED: [2015-12-07 Mon 22:30]
#+BEGIN_EXAMPLE
 [12/7/15, 6:58:02 PM] kungchaowang: there are two parameters in elastic search:

1. network.bind_host : specify which ip to bind, can be IPv4 or IPv6, but if not specified, it's default to 192.168.0.1

2. network.publish_host : set the address other nodes will use to communicate to this node, if not specified, default to 192.168.0.1

So, if you only set publish_host to IP other than 192.168.0.1, then you will have trouble, because you can bin to 192.168.0.1, but ask others to communicate to you using different IP

BUT!, there is a third parameter called "network.host", if you set this, you are effectively setting both network.publish_host and network.bind_host altogether! So, if you like to prevent from network error, this is the most easy go to settings for convenience.

If I did not explain it the way you can understand it, please let me know.
 [12/7/15, 7:16:39 PM] denny: ok,thanks
 [12/7/15, 7:16:50 PM] kungchaowang: make sense to you?
 [12/7/15, 7:18:32 PM] kungchaowang: don't feel bad not knowing this, as I am working more closely to elastic search than chef. For chef, a lot of things I don't know, but for elastic search, you can ask me for questions
#+END_EXAMPLE
** DONE audit elasticsearch initscript issue in digitalocean
   CLOSED: [2016-03-14 Mon 14:09]
http://123.57.240.189:48080/job/DigitalOceanDeployCookbooks/147/console
#!/bin/bash -ex
. /lib/lsb/init-functions
pid=`pidofproc -p $PID_FILE elasticsearch_audit`

#+BEGIN_EXAMPLE
root@kitchen-autotest-3nodes-node2:/var/log/elasticsearch# cat /var/log/elasticsearch/kitchen-autotest-3nodes-mdm-audit.log
[2016-03-14 02:00:54,743][INFO ][node                     ] [node2-ubuntu-1404-cluster] version[2.1.1], pid[23253], build[40e2c53/2015-12-15T13:05:55Z]
[2016-03-14 02:00:54,743][INFO ][node                     ] [node2-ubuntu-1404-cluster] initializing ...
[2016-03-14 02:00:54,944][INFO ][plugins                  ] [node2-ubuntu-1404-cluster] loaded [], sites [head]
[2016-03-14 02:00:54,966][ERROR][bootstrap                ] Exception
java.lang.IllegalStateException: Failed to obtain node lock, is the following location writable?: [/usr/share/elasticsearch/kitchen-autotest-3nodes-mdm-audit]
        at org.elasticsearch.env.NodeEnvironment.<init>(NodeEnvironment.java:178)
        at org.elasticsearch.node.Node.<init>(Node.java:153)
        at org.elasticsearch.node.Node.<init>(Node.java:128)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)

root@kitchen-autotest-3nodes-node2:/var/log/elasticsearch# ls -lth /usr/share/elasticsearch/kitchen-autotest-3nodes-mdm-audit/nodes/0
total 8.0K
drwxrwxr-x 2 elasticsearch elasticsearch 4.0K Mar 14 02:00 _state
drwxrwxr-x 4 elasticsearch elasticsearch 4.0K Mar 13 22:55 indices
-rw-rw-r-- 1 elasticsearch elasticsearch    0 Mar 13 22:52 node.lock
#+END_EXAMPLE
** DONE [#A] elasticsearch: discovery.zen.ping.unicast.hosts
   CLOSED: [2016-05-21 Sat 13:36]
https://github.com/elastic/elasticsearch/issues/3745

discovery.zen.ping.unicast.hosts is used only during initial
discovery, when nodes starts up and when discovery is restarted if the
number of master eligible nodes falls bellow minimum_master_nodes
setting. In your case, since you have only one master eligible node,
once this node has started this list is no longer needed because the
list of nodes in the cluster is maintained by the master.

In general, there is typically no need to restart nodes after changes
are made in discovery.zen.ping.unicast.hosts because this list doesn't
have to be comprehensive for the nodes to discovery each other.
** DONE elasticsearch close and open index: curl -XPOST $host:$port/$index/_close
   CLOSED: [2016-08-05 Fri 10:18]
curl -XPOST http://all-in-one-DockerDeployAllInOne-32:9200/master-index-e4010da4110ba377d100f050cb4440db/_close
curl -XPOST http://all-in-one-DockerDeployAllInOne-32:9200/master-index-e4010da4110ba377d100f050cb4440db/_open
#+BEGIN_EXAMPLE
root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs# curl -XGET 'http://all-in-one-DockerDeployAllInOne-32:9200/master-index-e4010da4110ba377d100f050cb4440db/_search?search_type=scan&scroll=10m&size=50' -d '
{
    "query" : {
        "match_all" : {}
    }
}'

<db/_search?search_type=scan&scroll=10m&size=50' -d '
> {
>     "query" : {
>         "match_all" : {}
>     }
> }'
{"_scroll_id":"c2NhbjsxOzU1MjQ6alA1MURIdENTazZjMWdrYTFRV01NQTsxO3RvdGFsX2hpdHM6MzI0Ow==","took":4,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":324,"max_score":0.0,"hits":[]}}root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs#
root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs# curl -XPOST http://all-in-one-DockerDeployAllInOne-32:9200/master-index-e4010da4110ba377d100f050cb4440db/_close
<-32:9200/master-index-e4010da4110ba377d100f050cb4440db/_close
{"acknowledged":true}root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs# curl -XGET 'http://all-in-one-DockerDeployAllInOne-32:9200/master-index-e4010da4110ba377d100f050cb4440db/_search?search_type=scan&scroll=10m&size=50' -d '
{
    "query" : {
        "match_all" : {}
    }
}'

<db/_search?search_type=scan&scroll=10m&size=50' -d '
> {
>     "query" : {
>         "match_all" : {}
>     }
> }'
{"error":{"root_cause":[{"type":"index_closed_exception","reason":"closed","index":"master-index-e4010da4110ba377d100f050cb4440db"}],"type":"index_closed_exception","reason":"closed","index":"master-index-e4010da4110ba377d100f050cb4440db"},"status":403}root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs#
root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs# curl -XPOST http://all-in-one-DockerDeployAllInOne-32:9200/master-index-e4010da4110ba377d100f050cb4440db/_open
<-32:9200/master-index-e4010da4110ba377d100f050cb4440db/_open
{"acknowledged":true}root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs# curl -XGET 'http://all-in-one-DockerDeployAllInOne-32:9200/master-index-e4010da4110ba377d100f050cb4440db/_search?search_type=scan&scroll=10m&size=50' -d '
{
    "query" : {
        "match_all" : {}
    }
}'

<db/_search?search_type=scan&scroll=10m&size=50' -d '
> {
>     "query" : {
>         "match_all" : {}
>     }
> }'
{"_scroll_id":"c2NhbjsxOzU1MjY6alA1MURIdENTazZjMWdrYTFRV01NQTsxO3RvdGFsX2hpdHM6MzI0Ow==","took":2,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":324,"max_score":0.0,"hits":[]}}root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs#
root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs#
#+END_EXAMPLE
** DONE elasticsearch delete index: curl -XDELETE 'http://localhost:9200/twitter/'
   CLOSED: [2016-08-05 Fri 10:12]
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-delete-index.html
http://stackoverflow.com/questions/22924300/removing-data-from-elasticsearch
#+BEGIN_EXAMPLE
root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs# curl http://all-in-one-DockerDeployAllInOne-32:9200/_cat/indices?v1
<# curl http://all-in-one-DockerDeployAllInOne-32:9200/_cat/indices?v1
green open staging-index-13a1f8adbec032ed68f3d035449ef48d 1 0      0 0    159b    159b
green open master-index-13a1f8adbec032ed68f3d035449ef48d  1 0      1 0   8.2kb   8.2kb
green open master-index-46078234297e400a1648d9c427dc8c4b  1 0      3 0  22.7kb  22.7kb
green open staging-index-e4010da4110ba377d100f050cb4440db 1 0 114022 0 145.4mb 145.4mb
green open staging-index-46078234297e400a1648d9c427dc8c4b 1 0  51166 5  65.1mb  65.1mb
green open master-index-e4010da4110ba377d100f050cb4440db  1 0    412 0 709.3kb 709.3kb
green open master-index-8cd6e43115e9416eb23609486fa053e3  1 0    406 0 712.3kb 712.3kb
root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs#  curl -XDELETE 'http://all-in-one-DockerDeployAllInOne-32:9200/master-index-e4010da4110ba377d100f050cb4440db/'
<nOne-32:9200/master-index-e4010da4110ba377d100f050cb4440db/'
{"acknowledged":true}root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs# curl http://all-in-one-DockerDeployAllInOne-32:9200/_cat/indices?v1
<# curl http://all-in-one-DockerDeployAllInOne-32:9200/_cat/indices?v1
green open staging-index-13a1f8adbec032ed68f3d035449ef48d 1 0      0 0    159b    159b
green open staging-index-e4010da4110ba377d100f050cb4440db 1 0 114022 0 145.4mb 145.4mb
green open master-index-13a1f8adbec032ed68f3d035449ef48d  1 0      1 0   8.2kb   8.2kb
green open master-index-46078234297e400a1648d9c427dc8c4b  1 0      3 0  22.7kb  22.7kb
green open staging-index-46078234297e400a1648d9c427dc8c4b 1 0  51166 5  65.1mb  65.1mb
green open master-index-8cd6e43115e9416eb23609486fa053e3  1 0    406 0 712.3kb 712.3kb
root@all-in-one-DockerDeployAllInOne-32:/opt/mdm/logs#
#+END_EXAMPLE
** TODO python create a code block of comment
#+BEGIN_SRC python
def create_es_repository():
    # curl -X PUT http://all-in-one-DockerDeployAllInOne-32:9200/my_snapshot/my_backup -d '{
    #     "type": "fs",
    #     "settings": {
    #         "location": "/data/backup/elasticsearch",
    #         "compress": true,
    #         "chunk_size": "10m"
    #     }
    # }'
    return
#+END_SRC
** TODO backup: http://104.131.129.100:48080/job/DockerDeployAllInOne/31/console
git@bitbucket.org:lrpdevops/mdmdevops-totvslabs.git

cd /var/lib/jenkins/code/DockerDeployAllInOne/sprint-38-backup/mdmdevops-totvslabs/cookbooks/all-in-one
export INSTANCE_NAME=all-in-one-DockerDeployAllInOne-31
export KITCHEN_YAML=.kitchen.yml

export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true
export CLEAN_START=false
export ENABLE_MORE_TEST=false
export PACKAGE_URL='http://172.17.0.2:18000'
export APP_BRANCH_NAME=sprint-38
export FRAMEWORK_BRANCH_NAME=sprint-38

export branch_name=sprint-38-backup
** DONE elasticsearch import and export by python client library: https://github.com/import-io/es-backup-scripts
   CLOSED: [2016-08-05 Fri 17:11]
** DONE elasticsearch fail to start: Unable to access 'path.scripts'
   CLOSED: [2016-09-30 Fri 10:15]
https://github.com/docker-library/docs/issues/378
https://github.com/dokku/dokku-elasticsearch/issues/23

#+BEGIN_EXAMPLE
root@mytest:/opt/elasticsearch# sudo -u elasticsearch JAVA_HOME=/opt/jdk/jre /opt/elasticsearch/bin/elasticsearch -p /var/run/elasticsearch/elasticsearch.pid -Des.default.path.home=/opt/elasticsearch -Des.default.path.logs=/opt/elasticsearch/logs -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/opt/elasticsearch/config
Exception in thread "main" java.lang.IllegalStateException: Unable to access 'path.scripts' (/opt/elasticsearch/config/scripts)
Likely root cause: java.nio.file.AccessDeniedException: /opt/elasticsearch/config/scripts
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
        at java.nio.file.Files.createDirectory(Files.java:674)
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
        at java.nio.file.Files.createDirectories(Files.java:767)
        at org.elasticsearch.bootstrap.Security.ensureDirectoryExists(Security.java:337)
        at org.elasticsearch.bootstrap.Security.addPath(Security.java:314)
        at org.elasticsearch.bootstrap.Security.addFilePermissions(Security.java:248)
        at org.elasticsearch.bootstrap.Security.createPermissions(Security.java:212)
        at org.elasticsearch.bootstrap.Security.configure(Security.java:118)
        at org.elasticsearch.bootstrap.Bootstrap.setupSecurity(Bootstrap.java:212)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:183)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:286)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.

#+END_EXAMPLE
** DONE [#A] elasticsearch delete/empty documents under index
   CLOSED: [2016-09-12 Mon 10:13]
http://stackoverflow.com/questions/23917327/delete-all-documents-from-index-type-without-deleting-type
I believe if you combine the delete by query with a match all it should do what you are looking for, something like this (using your example):

curl -XDELETE 'http://localhost:9200/twitter/tweet/_query' -d '{
    "query" : {
        "match_all" : {}
    }
}'
Or you could just delete the type:

curl -XDELETE http://localhost:9200/twitter/tweet
** #  --8<-------------------------- separator ------------------------>8--
** TODO [#A] What to handle, when elasticsearch health status is RED
https://www.elastic.co/guide/en/elasticsearch/reference/current/_cluster_health.html
** TODO [#A] elasticsearch clustering model
https://www.elastic.co/guide/en/elasticsearch/guide/current/_an_empty_cluster.html
- As nodes are added to or removed from the cluster, the cluster
  reorganizes itself to spread the data evenly.

- One node in the cluster is elected to be the master node, which is
  in charge of managing cluster-wide changes like creating or deleting
  an index, or adding or removing a node from the cluster.

- The master node does not need to be involved in document-level changes or searches, which means that having just one master node will not become a bottleneck as traffic grows.

- Any node can become the master. Our example cluster has only one node, so it performs the master role.

- As users, we can talk to any node in the cluster, including the
  master node. Every node knows where each document lives and can
  forward our request directly to the nodes that hold the data we are
  interested in.
** TODO [#A] elasticsearch Marvel for Monitoring
https://www.elastic.co/guide/en/elasticsearch/guide/current/_marvel_for_monitoring.html
** TODO elasticsearch create index and data
https://www.elastic.co/guide/en/elasticsearch/guide/current/create-doc.html

curl -XPUT http://833575d34be3:9200/website/blog/123?op_type=create -d "{ \"field\" : \"value\"}"
** TODO manually start elasticsearch service without linux initscript
su elasticsearch

/usr/lib/jvm/java-8-oracle-amd64/bin/java -Xms512m -Xmx512m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -server -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Xms512m -Xmx512m -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.1.1.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch/elasticsearch.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/usr/share/elasticsearch --default.path.conf=/etc/elasticsearch
** #  --8<-------------------------- separator ------------------------>8--
** TODO [#A] elasticsearch How to scale for rebalance data?
https://www.elastic.co/guide/en/elasticsearch/guide/current/_scale_horizontally.html
** TODO [#A] What to handle, when elasticsearch health status is RED
https://www.elastic.co/guide/en/elasticsearch/reference/current/_cluster_health.html
** TODO [#A] detect elasticsearch cluster low capacity event      :IMPORTANT:
"mem":{"total_in_bytes":33574821888}
"mem":{"heap_used_in_bytes":284325600,"Heap_max_in_bytes":519045120}
"fs":{"total_in_bytes":1937945255936,"free_in_bytes":936738643968,"available_in_bytes":838249472000}

284,325,600
519,045,120

33,574,821,888
1,937,945,255,936

936738643968

838,249,472,000

#  --8<-------------------------- separator ------------------------>8--

curl '127.0.0.1:9200/_cat/allocation?v'
https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-allocation.html

- disk, ram
- docker resource utilization doesn't make sense
*** curl '127.0.0.1:9200/_cat/allocation?v'
#+BEGIN_EXAMPLE
root@8a249c0377a1:~#  curl '127.0.0.1:9200/_cat/allocation?v'
 curl '127.0.0.1:9200/_cat/allocation?v'
shards disk.used disk.avail disk.total disk.percent host         ip          node
     2   404.7gb    497.6gb    902.4gb           44 8a249c0377a1 172.17.2.46 master-ubuntu-1404
     2   404.7gb    497.6gb    902.4gb           44 c1650bfde8f7 172.17.2.47 slavenode-ubuntu-1404
     1                                                                       UNASSIGNED
#+END_EXAMPLE
*** curl http://localhost:9200/_cluster/stats
#+BEGIN_EXAMPLE
Denny-mac:org_data mac$ curl http://localhost:9200/_cluster/stats
{"timestamp":1441812608075,"cluster_name":"mdm","status":"green","indices":{"count":3,"shards":{"total":14,"primaries":7,"replication":1.0,"index":{"shards":{"min":2,"max":10,"avg":4.666666666666667},"primaries":{"min":1,"max":5,"avg":2.3333333333333335},"replication":{"min":1.0,"max":1.0,"avg":1.0}}},"docs":{"count":3170,"deleted":0},"store":{"size_in_bytes":23219453,"throttle_time_in_millis":4264},"fielddata":{"memory_size_in_bytes":0,"evictions":0},"filter_cache":{"memory_size_in_bytes":0,"evictions":0},"id_cache":{"memory_size_in_bytes":0},"completion":{"size_in_bytes":0},"segments":{"count":15,"memory_in_bytes":3234414,"index_writer_memory_in_bytes":152404,"index_writer_max_memory_in_bytes":106807296,"version_map_memory_in_bytes":0,"fixed_bit_set_memory_in_bytes":0},"percolate":{"total":0,"time_in_millis":0,"current":0,"memory_size_in_bytes":-1,"memory_size":"-1b","queries":0}},"nodes":{"count":{"total":2,"master_only":0,"data_only":0,"master_data":2,"client":0},"versions":["1.7.1"],"os":{"available_processors":16,"mem":{"total_in_bytes":33574821888},"cpu":[{"vendor":"Intel","model":"Core(TM) i7-3770 CPU @ 3.40GHz","mhz":3401,"total_cores":8,"total_sockets":8,"cores_per_socket":16,"cache_size_in_bytes":8192,"count":2}]},"process":{"cpu":{"percent":14},"open_file_descriptors":{"min":278,"max":285,"avg":281}},"jvm":{"max_uptime_in_millis":4119005,"versions":[{"version":"1.8.0_40","vm_name":"Java HotSpot(TM) 64-Bit Server VM","vm_version":"25.40-b25","vm_vendor":"Oracle Corporation","count":2}],"mem":{"heap_used_in_bytes":328957200,"heap_max_in_bytes":519045120},"threads":187},"fs":{"total_in_bytes":1937945255936,"free_in_bytes":1167519952896,"available_in_bytes":1069030780928},"plugins":[{"name":"head","version":"NA","description":"No description found.","url":"/_plugin/head/","jvm":false,"site":true},{"name":"marvel","version":"1.3.1","description":"Elasticsearch Management & Monitoring","url":"/_plugin/marvel/","jvm":true,"site":true}]}}Denny-mac:org_data mac$

{"timestamp":1441812608075,"cluster_name":"mdm",
"status":"green",
"indices":{"count":3,"shards":{"total":14,"primaries":7,"replication":1.0,
    "index":{"shards":{"min":2,"max":10,"avg":4.666666666666667},
             "primaries":{"min":1,"max":5,"avg":2.3333333333333335},
             "replication":{"min":1.0,"max":1.0,"avg":1.0}}},
             "docs":{"count":3170,"deleted":0},
             "store":{"size_in_bytes":23219453,"throttle_time_in_millis":4264},
             "fielddata":{"memory_size_in_bytes":0,"evictions":0},
             "filter_cache":{"memory_size_in_bytes":0,"evictions":0},
             "id_cache":{"memory_size_in_bytes":0},
             "completion":{"size_in_bytes":0},
             "segments":{"count":15,"memory_in_bytes":3234414,
                         "index_writer_memory_in_bytes":152404,
                         "index_writer_max_memory_in_bytes":106807296,
                         "version_map_memory_in_bytes":0,
                         "fixed_bit_set_memory_in_bytes":0},
             "percolate":{"total":0,"time_in_millis":0,"current":0,"memory_size_in_bytes":-1,"memory_size":"-1b","queries":0}},
             "nodes":{"count":{"total":2,"master_only":0,"data_only":0,"master_data":2,"client":0},
             "versions":["1.7.1"],
             "os":{"available_processors":16,"mem":{"total_in_bytes":33574821888},
             "cpu":[{"vendor":"Intel","model":"Core(TM) i7-3770 CPU @ 3.40GHz","mhz":3401,"total_cores":8,"total_sockets":8,"cores_per_socket":16,"cache_size_in_bytes":8192,"count":2}]},
             "process":{"cpu":{"percent":14},
             "open_file_descriptors":{"min":278,"max":285,"avg":281}},
             "jvm":{"max_uptime_in_millis":4119005,
                   "versions":[{"version":"1.8.0_40","vm_name":"Java HotSpot(TM) 64-Bit Server VM","vm_version":"25.40-b25","vm_vendor":"Oracle Corporation","count":2}],
                   "mem":{"heap_used_in_bytes":328957200,"heap_max_in_bytes":519045120},"threads":187},
             "fs":{"total_in_bytes":1937945255936,"free_in_bytes":1167519952896,"available_in_bytes":1069030780928},
             "plugins":[{"name":"head","version":"NA","description":"No description found.","url":"/_plugin/head/","jvm":false,"site":true},{"name":"marvel","version":"1.3.1","description":"Elasticsearch Management & Monitoring","url":"/_plugin/marvel/","jvm":true,"site":true}]}}
#+END_EXAMPLE
*** curl "127.0.0.1:9200/_cat/nodes?v&h=id,ip,port,m,disk.avail,heap.percent,ram.percent,ram.max"
#+BEGIN_EXAMPLE
root@8a249c0377a1:~# curl "127.0.0.1:9200/_cat/nodes?v&h=id,ip,port,m,disk.avail,heap.percent,ram.percent,ram.max"
<v&h=id,ip,port,m,disk.avail,heap.percent,ram.percent,ram.max"
id   ip          port m disk.avail heap.percent ram.percent ram.max
hqsH 172.17.2.46 9300 m    497.6gb           54          45  15.6gb
FNXZ 172.17.2.47 9300 *    497.6gb           61          45  15.6gb
#+END_EXAMPLE
** TODO [#A] elasticsearch backup
root@mytest-kitchen-node4:/#

curl -XPUT http://kitchen-autotest-6nodes-node3:9200/_snapshot/my_backup -d '{ "type": "fs", "settings": { "location": "/data/backup"}}'

<: "fs", "settings": { "location": "/data/backup"}}'
{"error":"RemoteTransportException[[node3-ubuntu-1404][inet[/172.17.0.6:9300]][cluster:admin/repository/put]]; nested: RepositoryVerificationException[[my_backup] [okHZiN86SQaRp9VMeIRsXQ, 'RemoteTransportException[[node4-ubuntu-1404][inet[/172.17.0.7:9300]][internal:admin/repository/verify]]; nested: RepositoryVerificationException[[my_backup] a file written by master to the store [/data/backup] cannot be accessed on the node [[node4-ubuntu-1404][okHZiN86SQaRp9VMeIRsXQ][mytest-kitchen-node4][inet[mytest-kitchen-node4/172.17.0.7:9300]]{max_local_storage_nodes=1}]. This might indicate that the store [/data/backup] is not shared between this node and the master node or that permissions on the store don't allow reading files written by the master node]; ']]]; ","status":500}root@mytest-kitchen-node4:/#

#+BEGIN_EXAMPLE
root@mytest-kitchen-node4:/# grep node4 /usr/local/etc/elasticsearch/elasticsearch.yml
<# grep node4 /usr/local/etc/elasticsearch/elasticsearch.yml
node.name: node4-ubuntu-1404
network.host: mytest-kitchen-node4
discovery.zen.ping.unicast.hosts: mytest-kitchen-node3,mytest-kitchen-node4
#+END_EXAMPLE
** TODO [#A] reconfigure elasticsearch configuration and data path
** TODO [#A] confusion about the data of elasticsearch
mkdir -p /home/denny/20160311
cp  -r /usr/share/elasticsearch/kitchen-cluster-mdm/nodes  /home/denny/20160311
ls -lth /usr/share/elasticsearch/kitchen-cluster-mdm/nodes

mkdir -p /home/denny/20160311-old
cp -r /usr/local/var/data/elasticsearch /home/denny/20160311-old

ls -lth /usr/share/elasticsearch/
ls -lth /usr/local/var/data/elasticsearch

du -h -d 1 /usr/local/var/data/elasticsearch
** TODO [#A] elasticsearch enable username/password for rest api and cluster autojoin
** TODO what does source means in elasticsearch documents: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-explain.html
** TODO version of search hit: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-version.html
** #  --8<-------------------------- separator ------------------------>8--
** web page: Elasticsearch Queries: A Thorough Guide - Logz.io
http://site2.logz.io/blog/elasticsearch-queries/
*** webcontent                                                     :noexport:
#+begin_example
Location: http://site2.logz.io/blog/elasticsearch-queries/
*
Logz.io Logz.io Logz.io Logz.io

  * PRODUCT
      + New – Cognitive Insights
      + Enterprise-Grade ELK
      + Use Cases
      + How it Works
      + Security & Compliance
      + AWS Analytics
      + ELK Apps
  * PRICING
      + Plans
      + FAQ
  * CUSTOMERS
      + Case Studies
      + Our Customers
  * RESOURCES
      + Learn
      + Blog
      + AWS Log Analysis Guide
      + The Complete ELK Stack Guide
      + Support
  * ABOUT US
      + Who We Are
      + Management Team
      + Partners
      + News & Events
      + Contribute to Our Blog
      + Careers
      + Contact Us
  * REQUEST DEMO
  * searchIcon-01Search
  * LOGIN
  * FREE TRIAL

FREE TRIAL
Previous Next

Elasticsearch Queries: A Thorough Guide

By Jurgens du Toit| 2016-09-05T10:27:48+00:00 April 7th, 2016|Blog, Elasticsearch|

elasticsearch queries

Even though search is the primary function of Elasticsearch, getting search right can be tough and
sometimes even confusing. To help, this guide will take you through the ins and outs of search
queries and set you up for future searching success.

Lucene queries

Elasticsearch is part of the ELK Stack and is built on Lucene, the search library from Apache, and
exposes Lucene's query syntax. It's such an integral part of Elasticsearch that when you query the
root of an Elasticsearch cluster, it will tell you the Lucene version:

{
"status" : 200,
"name" : "Ikthalon",
"cluster_name" : "elasticsearch",
"version" : {
"number" : "1.7.5",
"build_hash" : "00f95f4ffca6de89d68b7ccaf80d148f1f70e4d4",
"build_timestamp" : "2016-02-02T09:55:30Z",
"build_snapshot" : false,
<b>"lucene_version" : "4.10.4"</b>
},
"tagline" : "You Know, for Search"
}

Knowing the Lucene syntax and operators will go a long way in helping you build queries. It's used
in both the simple and the standard query string query. Here's some of the basics:

Boolean Operators

As with most computer languages, Elasticsearch supports the AND, OR, and NOT operators:

  * jack AND jill - Will return events that contain both jack and jill
  * ahab NOT moby - Will return events that contain ahab but not moby
  * tom OR jerry - Will return events that contain tom or jerry, or both

Fields

You might be looking for events where a specific field contains certain terms. You specify that as
follows:

  * name:"Ned Stark"

Be careful with values with spaces such as "Ned Stark." You'll need to enclose it in double quotes
to ensure that the whole value is used.

Ranges

You can search for fields within a specific range, using square brackets for inclusive range
searches and curly braces for exclusive range searches:

  * age:[3 TO 10] - Will return events with age between 3 and 10
  * price:{100 TO 400} - Will return events with prices between 101 and 399
  * name: [Adam TO Ziggy] - Will return names between and including Adam and Ziggy

As you can see in the examples above, you can use ranges in non-numerical fields like strings and
dates as well.

Wildcards, Regexes and Fuzzy Searching

Search would not be search without wildcards. You can use the * character for multiple character
wildcards or the ? character for single character wildcards:

  * Ma?s - Will match Mars, Mass, and Maps
  * Ma*s - Will match Mars, Matches, and Massachusetts

Regexes give you even more power. Just place your regex between forward slashes (/):

  * /m[ea]n/ - Will match both pen and pan
  * /<.+>/ - Will match text that resembles an HTML tag

Fuzzy searching uses the Damerau-Levenshtein Distance to match terms that are similar in spelling.
This is great when your data set has misspelled words. Use the tilde (~) along with a number to
specify the how big the distance between words can be:

  * john~2 - Will match, amongst others, jean, johns, jhon, and horn

URI Search

The easiest way to search your Elasticsearch cluster is through URI search. You can pass a simple
query to Elasticsearch using the q query parameter. The following query will search your whole
cluster for documents with a name field equal to "travis":

  * curl "localhost:9200/_search?q=name:travis"

With the Lucene syntax, you can build quite impressive searches. Usually you'll have to URL-encode
characters such as spaces (it's been omitted in these examples for clarity):

  * curl "localhost:9200/_search?q=name:john~1 AND (age:[30 TO 40} OR surname:K*) AND -city"

A number of options are available that allow you to customize the URI search, specifically in terms
of which analyzer to use (analyzer), whether the query should be fault-tolerant (lenient), and
whether an explanation of the scoring should be provided (explain).

Although the URI search is a simple and efficient way to query your cluster, you'll quickly find
that it doesn't support all of the features offered to you by Elasticsearch. The full power of
Elasticsearch is exposed through Request Body Search. Using Request Body Search allows you to build
a complex search request using various elements and query clauses that will match, filter, and
order as well as manipulate documents based on multiple criteria.

The Request Body Search

Request Body Search uses a JSON document that contains various elements to create a search on your
Elasticsearch cluster. Not only can you specify search criteria, you can also specify the range and
number of documents that you expect back, the fields that you want, and various other options.

The first element of a search is the query element that uses Query DSL. Using Query DSL can
sometimes be confusing because the DSL can be used to combine and build up query clauses into a
query that can be nested deeply. Since most of the Elasticsearch documentation only refers to
clauses in isolation, it's easy to lose sight of where clauses should be placed.

To use the Query DSL, you need to include a "query" element in your search body and populate it
with a query built using the DSL:

{"query": { "match": { "_all": "meaning" } } }

In this case, the "query" element contains a "match" query clause that looks for the term "meaning"
in all of the fields in all of the documents in your cluster.

The query element is used along with other elements in the search body:

{
"query": {
"match": { "_all": "meaning" }
},
"fields": ["name", "surname", "age"],
"from": 100, "size": 20
}

Here, we're using the "fields" element to restrict which fields should be returned and the "from"
and "size" elements to tell Elasticsearch we're looking for documents 100 to 119 (starting at 100
and counting 20 documents).

The Query DSL

The Query DSL can be invoked using most of Elasticsearch's search APIs. For simplicity, we'll look
only at the Search API that uses the _search endpoint. When calling the search API, you can specify
the index and / or type on which you want to search. You can even search on multiple indices and
types by separating their names with commas or using wildcards to match multiple indices and types:

Search on all the Logstash indices:

  * curl localhost:9200/logstash-*/_search

Search in the current and legacy indices, in the documents type:

  * curl localhost:9200/current,legacy/documents/_search

Search in the clients indices, in the bigcorp and smallco types:

  * curl localhost:9200/clients/bigcorp,smallco/_search

We'll be using Request Body Searches, so searches should be invoked as follows:

  * curl localhost:9200/_search -d '{"query":{"match": {"_all":"meaning"}}}'

Compound Queries

Although there are multiple query clause types, the one you'll use the most is Compound Queries
because it's used to combine multiple clauses to build up complex queries.

The Bool Query is probably used the most because it can combine the features of some of the other
compound query clauses such as the And, Or, Filter, and Not clauses. It is used so much that these
four clauses have been deprecated in various versions in favor of using the Bool query. Using it is
best explained with an example:

curl localhost:9200/_search -d '{
"query":{
"bool": {
"must": {
"fuzzy" : {
"name": "john",
"fuzziness": 2
}
},
"must_not": {
"match": {
"_all": "city"
}
},
"should": [
{
"range": {
"age": { "from": 30, "to": 40 }
}
},
{
"wildcard" : { "surname" : "K*" }
}
]
}
}
}'

Within the query element, we've added the bool clause that indicates that this will be a boolean
query. There's quite a lot going in there, so let's cover it clause-by-clause, starting at the top:

must

All queries within this clause must match a document for it to be returned by Elasticsearch. Think
of this as your AND queries. The query we used here is the fuzzy query, and it will match any
documents that have a name field that matches "john" in a fuzzy way. The extra "fuzziness"
parameter tells Elasticsearch that it should be using a Damerau-Levenshtein Distance of 2 two
determine the fuzziness.

must_not

Any documents that match the query within this clause will be excluded from the result set. This is
the NOT or minus (-) operator of the query DSL. In this case, we do a simple match query, looking
for documents that contain the term "city." Using _all as the field name indicates that the term
can appear in any of the document's fields. This is the must_not clause, so matching documents will
be excluded.

should

Up until now, we have been dealing with absolutes: must and must_not. Should is less absolute and
is equivalent to the OR operator. Elasticsearch will return any documents that match one or more of
the queries in the should clause. The first query that we provided looks for documents where the
age field is between 30 and 40. The second query does a wildcard search on the surname field,
looking for values that start with "K."

The query contained three different clauses, so Elasticsearch will only return documents that match
the criteria in all of them. These queries can be nested, so you can build up very complex queries
by specifying a bool query as a must, must_not, should or filter query.

filter

One clause type we have not discussed for a compound query is the filter clause. Here is an example
where we use one:

curl localhost:9200/_search -d '{
"query":{
"bool": {
"must": {
{ "match_all": {} }
},
"filter": {
"term": {
"email": "joe@bloggs.com"
}
}
}
}
}`

The match_all query in the must clause tells Elasticsearch that it should return all of the
documents. This might not seem to be a very useful search, but it comes in handy when you use it in
conjunction with a filter as we have done here. The filter we have specified is a term query,
asking for all documents that contain an email field with the value "joe@bloggs.com." We have used
a filter to specify which documents we want, so they will all be returned with a score of 1.
Filters are not used in the calculation of scores, so the match_all query gives all documents a
score of 1.

One thing to note is that this query will not work as expected if the email field is analyzed,
which is the default for fields in Elasticsearch. The reason behind this is a topic best discussed
in another blog post, but it comes down to the fact that Elasticsearch analyzes both fields and
queries when they come in. In this case, the email field will be broken up into three parts: joe,
bloggs, and com. This means that it will match searches and documents for any three of those terms.

Filters Versus Queries

People who have used Elasticsearch before version 2 will be familiar with filters and queries. You
used to build up a query body using both filters and queries. The difference between the two was
that filters were generally faster because they check only if a document matches at all and not
whether it matches well. In other words, filters give a boolean answer whereas queries return a
calculated score of how well a document matches a query. Various performance enhancements were
associated with filters due to their simplified nature.

Since version 2 of Elasticsearch, filters and queries have been merged and any query clause can be
used as either a filter or a query (depending on the context). As with version 1, filters are
cached and should be used if scoring does not matter.

Scoring

We have mentioned the fact that Elasticsearch returns a score along with all of the matching
documents from a query:

> curl "localhost:9200/_search?q=application"
{
"_shards":{
"total" : 5,
"successful" : 5,
"failed" : 0
},
"hits":{
"total" : 1,
"max_score": 2.3,
"hits" : [
{
"_index" : "logstash-2016.04.04",
"_type" : "logs",
"_id" : "1",
"_score": 2.3,
"_source" : {
"message" : "Log message from my application"
}
}
]
}
}

This score is calculated against the documents in Elasticsearch based on the provided queries.
Factors such as the length of a field, how often the specified term appears in the field, and (in
the case of wildcard and fuzzy searches) how closely the term matches the specified value all
influence the score. The calculated score is then used to order documents, usually from the highest
score to lowest, and the highest scoring documents are then returned to the client. There are
various ways to influence the scores of different queries such as the boost parameter. This is
especially useful if you want certain queries in a complex query to carry more weight than others
and you are looking for the most significant documents.

When using a query in a filter context (as explained earlier), no score is calculated. This
provides the enhanced performance usually associated with using filters but does not provide the
ordering and significance features that comes with scoring.

Conclusion

The hardest thing about Elasticsearch is the depth and breadth of the available features. We have
tried to cover the essential elements in as much detail as possible without drowning you in
information. Ask any questions you might have in the comments, and look out for more in-depth posts
covering some of the features we have mentioned. You can also read my prior Elasticsearch tutorial
to learn more.

Logz.io is a predictive, cloud-based log management platform that is built on top of the
open-source ELK Stack, which is comprised of Elasticsearch, Logstash, and Kibana. Start your free
trial today!
[9c0d5f550c0]
Jurgens du Toit
Jurgens tries to write good code for a living. He even succeeds at it sometimes. When he isn't
writing code, he's wrangling data as a hobby. Sometimes the data wins, but we don't talk about
that. Ruby and Elasticsearch are his weapons of choice, but his ADD always allows for new
interests. He's also the community maintainer for a number of Logstash inputs.

FacebookTwitterLinkedinGoogleplus

Related Posts

  * Solr vs. Elasticsearch: Who's The Leading Open Source Search Engine?

    Solr vs. Elasticsearch: Who's The Leading Open Source Search Engine?

    September 27th, 2016 | 0 Comments
  * Why We Built Our Log Analytics SaaS on AWS

    Why We Built Our Log Analytics SaaS on AWS

    September 26th, 2016 | 0 Comments
  * Monitoring Magento Visitor Logs with the ELK Stack

    Monitoring Magento Visitor Logs with the ELK Stack

    September 21st, 2016 | 0 Comments
  * Building an NGINX Access Log Monitoring Dashboard

    Building an NGINX Access Log Monitoring Dashboard

    September 20th, 2016 | 0 Comments
  * Docker Swarm Monitoring and Logging Using the ELK Stack

    Docker Swarm Monitoring and Logging Using the ELK Stack

    September 15th, 2016 | 0 Comments
  * Installing the ELK Stack on Windows

    Installing the ELK Stack on Windows

    September 13th, 2016 | 0 Comments
  * How We Monitor Elasticsearch with Graphite and Grafana

    How We Monitor Elasticsearch with Graphite and Grafana

    September 8th, 2016 | 0 Comments
  * 5 Features We've Added to Kibana

    5 Features We've Added to Kibana

    September 7th, 2016 | 0 Comments
  * How to Log All UI Test Method Calls Easily with ECMA6 Javascript Proxies

    How to Log All UI Test Method Calls Easily with ECMA6 Javascript Proxies

    September 1st, 2016 | 0 Comments
  * Overcoming the Biggest Challenge in Log Analysis Using Logz.io Cognitive Insights

    Overcoming the Biggest Challenge in Log Analysis Using Logz.io Cognitive Insights

    August 31st, 2016 | 0 Comments
  * Amazon EC2 Container Service (ECS) Log Analysis

    Amazon EC2 Container Service (ECS) Log Analysis

    August 25th, 2016 | 0 Comments
  * Apache Log Analysis with Logz.io

    Apache Log Analysis with Logz.io

    August 22nd, 2016 | 0 Comments

Recent Posts

  * Solr vs. Elasticsearch: Who's The Leading Open Source Search Engine? September 27, 2016
  * Why We Built Our Log Analytics SaaS on AWS September 26, 2016
  * Monitoring Magento Visitor Logs with the ELK Stack September 21, 2016
  * Building an NGINX Access Log Monitoring Dashboard September 20, 2016

×

[popup]
[logo298x88]

Get up and running with production-level ELK in 5 minutes!

Start Your Free Trial

Full name [                    ]
Work email
Company [                    ]
Password [                    ]
Phone
By signing up you are accepting our Terms of Service
Please wait while we're setting up your environment...
 START YOUR TRIAL
What's included in the trial

  * Up to 1 GB of data per day
  * 3 day data retention
  * No credit card required!
  * 5 minute setup

×

Enterprise Plan Request

Thank you, we will contact you soon!

×

Request a Live Demo

Thank you, we will contact you soon!

×

Register to the Webinar

Thank you, a confirmation mail will be sent shortly!

PRODUCT

  * Hosted ELK as a Service
  * Enterprise-Grade ELK
  * Use Cases
  * How it Works
  * Security & Compliance
  * AWS Elasticsearch
  * Data Analytics for AWS

RESOURCES

  * Customers
  * Case Studies
  * Learn
  * The ELK Stack Guide
  * AWS Log Analysis
  * Blog
  * Support

PRICING

  * Plans
  * FAQs
  * Request a Live Demo

ABOUT US

  * Who We Are
  * Management Team
  * Partners
  * News & Events
  * Contact Us
  * Careers
  * Contribute to Our Blog

                                            Contact Us

[logo-footer]Privacy Policy | Terms Of Use | All rights Reserved by logz.io © 2016
×
[                    ]
  * ELK as a Service
  * Enterprise Grade ELK
  * Alerts
  * ELK Apps

  * Learn
  * Blog
  * Support
  * Guide to ELK

  * Pricing & Plans
  * Contact Us

Get the latest
DevOps news and tips
to your inbox

Thank you for subscribing!

We write about DevOps, Log Analytics, Elasticsearch, Logstash and Kibana.

×
# *

#+end_example
** web page: Elasticsearch Cheat Sheet for developers
http://elasticsearch-cheatsheet.jolicode.com/
*** webcontent                                                     :noexport:
#+begin_example
Location: http://elasticsearch-cheatsheet.jolicode.com/
[Elasticsea] [Elasticsea]

🔎 Elasticsearch Cheatsheet 🔍

All the API endpoints and pro-tips you always forgot about in one place!
Built by developers for developers. Hosted on GitHub, contributions welcome.

Links Queries Indexes Debug Cluster & Plugins
Elasticsearch 1.X Elasticsearch 2.X Elasticsearch 5.X
Elasticsearch 1.X
Elasticsearch 2.X
Elasticsearch 5.X

Elasticsearch 1.7 is scheduled for End Of Life at the release of Elasticsearch 5. Consider
upgrading. More information about supported versions.

Links

First thing, forget about your curl calls and install the Sense extension please !

First thing, forget about your curl calls and install Sense please !

  * Elasticsearch: The Definitive Guide, official book;
  * Elasticsearch: The Definitive Guide, official book;
  * Elasticsearch: The Definitive Guide, official book;
  * Elasticsearch Reference, official documentation;
  * Elasticsearch Reference, official documentation;
  * Elasticsearch Reference, official documentation;
  * Awesome Elasticsearch, curated list of resources about ES;
  * Found Play, JSFiddle-like playground for Elasticsearch;
  * Official forum and StackOverflow for support.

Queries

There is two syntax for the basic queries: a simple one on the left, where you can't use any
option, and an extended one on the right. Most of the beginner headache with the DSL come from
this:

GET _search
{
  "query": {
    "match": {
      "FIELD": "TEXT"
    }
  }
}

to

GET _search
{
  "query": {
    "match": {
      "FIELD": {
        "query": "TEXT",
        "OPTION": "VALUE"
      }
    }
  }
}

Full search example with aggregation, highlight, filter...

          GET /_search
{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "title": "smith"
              }
            }
          ],
          "must_not": [
            {
              "match_phrase": {
                "title": "granny smith"
              }
            }
          ]
        }
      },
      "filter": {
        "bool": {
          "must_not": [
            {
              "missing": {
                "field": "title"
              }
            }
          ]
        }
      }
    }
  },
  "aggs": {
    "my_agg": {
      "terms": {
        "field": "user",
        "size": 10
      }
    }
  },
  "highlight": {
    "pre_tags" : ["<em>"],
    "post_tags" : ["</em>"],
    "fields": {
      "body" : {
        "number_of_fragments": 1,
        "fragment_size": 20
      },
      "title": {}
    }
  },
  "size": 20,
  "from": 100,
  "_source": ["title", "id"],
  "sort": [
    { "_id" : {"order" : "desc"}}
  ]
}

          GET /_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "title": "smith"
          }
        }
      ],
      "must_not": [
        {
          "match_phrase": {
            "title": "granny smith"
          }
        }
      ],
      "filter": [
        {
          "exists": {
            "field": "title"
          }
        }
      ]
    }
  },
  "aggs": {
    "my_agg": {
      "terms": {
        "field": "user",
        "size": 10
      }
    }
  },
  "highlight": {
    "pre_tags": [
      "<em>"
    ],
    "post_tags": [
      "</em>"
    ],
    "fields": {
      "body": {
        "number_of_fragments": 1,
        "fragment_size": 20
      },
      "title": {}
    }
  },
  "size": 20,
  "from": 100,
  "_source": [
    "title",
    "id"
  ],
  "sort": [
    {
      "_id": {
        "order": "desc"
      }
    }
  ]
}

Common queries

"multi_match": {
  "query": "Elastic",
  "fields": ["user.*", "title^3"],
  "type": "best_fields"
}

"bool": {
  "must": [],
  "must_not": [],
  "filter": [],
  "should": [],
  "minimum_should_match" : 1
}

"bool": {
  "must": [],
  "must_not": [],
  "should": [],
  "minimum_should_match" : 1
}

"filtered": {
  "query": {
    "match_all": {}
  },
  "filter": {
    "range": {...}
  }
}

"range": {
  "age": {
    "gte": 10,
    "lte": 20,
    "boost": 2
  }
}

        QueryString syntax recap

          Search in the default _all field:

GET /_search?q=pony

Complex search with operator and exact phrase search with boost:

GET /_search?q=title:(joli OR code) AND author:"Damien Alexandre"^2

Search with wildcard and special queries:

GET /_search?q=_exists_:title OR title:singl? noneOrAnyChar*cter

Search with fuzzyness and range:

GET /_search?q=title:elastichurch~3 AND date:[2016-01-01 TO 2018-12-31]

Use in Query DSL (not recommended for user search):

GET /_search
{
  "query": {
    "query_string": {
      "default_field": "content",
      "query": "elastic AND (title:lucene OR title:solr)"
    }
  }
}

Indexes and mapping

Create an index with settings and mapping

PUT /my_index_name
{
  "settings": {
    "number_of_replicas": 1,
    "number_of_shards": 3,
    "analysis": {},
    "refresh_interval": "1s"
  },
  "mappings": {
    "my_type_name": {
      "properties": {
        "title": {
          "type": "string",
          "analyzer": "english"
        }
      }
    }
  }
}

PUT /my_index_name
{
  "settings": {
    "number_of_replicas": 1,
    "number_of_shards": 3,
    "analysis": {},
    "refresh_interval": "1s"
  },
  "mappings": {
    "my_type_name": {
      "properties": {
        "title": {
          "type": "text",
          "analyzer": "english"
        }
      }
    }
  }
}

Update index settings dynamically

PUT /my_index_name/_settings
{
  "index": {
    "refresh_interval": "-1",
    "number_of_replicas": 0
  }
}

Update an index by adding a field to a type

PUT /my_index_name/_mapping/my_type_name
{
  "my_type_name": {
    "properties": {
      "tag": {
        "type": "string",
        "index": "not_analyzed"
      }
    }
  }
}

PUT /my_index_name/_mapping/my_type_name
{
  "my_type_name": {
    "properties": {
      "tag": {
        "type": "keyword"
      }
    }
  }
}

Get the mapping and the settings

GET /my_index_name/_mapping

GET /my_index_name/_settings

Create a document (auto-generated ID)

POST /my_index_name/my_type_name
{
  "title": "Elastic is funny",
  "tag": [
    "lucene"
  ]
}

Create or update a document

PUT /my_index_name/my_type_name/12abc
{
  "title": "Elastic is funny",
  "tag": [
    "lucene"
  ]
}

Delete a document

DELETE /my_index_name/my_type_name/12abc

Open and close indexes to save memory and CPU

POST /my_index_name/_close

POST /my_index_name/_open

Remove and create aliases

POST /_aliases
{
  "actions": [
    {
      "remove": {
        "index": "my_index_name",
        "alias": "foo"
      }
    },
    {
      "add": {
        "index": "my_index_name",
        "alias": "bar",
        "filter" : { "term" : { "user" : "damien" } }
      }
    }
  ]
}

List aliases

GET /_aliases

GET /my_index_name/_alias/*

GET /*/_alias/*

GET /*/_alias/foo

Create and list index warmer

Create a query that will be played on every new document:

PUT /my_index_name/_warmer/my_warmer
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "costly_aggs": {
      "terms": {
        "field": "title"
      }
    }
  }
}

GET /my_index_name/_warmer

GET /my_index_name/_warmer/*

Indices monitoring and information

GET /my_index_name/_stats

GET /my_index_name/_stats

GET /my_index_name/_segments

GET /my_index_name/_recovery?pretty&human

GET /my_index_name/_warmer/*

Indices status and management

POST /my_index_name/_cache/clear

POST /my_index_name/_refresh

POST /my_index_name/_flush

POST /my_index_name/_optimize

POST /my_index_name/_forcemerge

POST /my_index_name/_upgrade

GET /my_index_name/_upgrade?pretty&human

Debug and development

Queries

Get a detailed view of what a query do:

GET /blog/post/_validate/query?explain
{
  "query": {
    "match": {
      "title": "Smith"
    }
  }
}

Get an explanation about a document matching or not:

GET /blog/post/1/_explain
{
  "query": {
    "match": {
      "title": "Smith"
    }
  }
}

Analysis

Test how a content is tokenized in a field:

GET /blog/_analyze?field=title&text=powerful

Test analyzer token output by analyzer:

GET /_analyze?analyzer=english&text=powerful

Cluster management and plugins

Cluster and node information

GET /_cluster/health?pretty

GET /_cluster/health?wait_for_status=yellow&timeout=50s

GET /_cluster/state

GET /_cluster/stats?human&pretty

GET /_cluster/pending_tasks

GET /_nodes

GET /_nodes/stats

GET /_nodes/nodeId1,nodeId2/stats

GET /_shutdown 😱

Moving shards manually

Ask the index my_index_name shard 0 of node1 to go to node2:

POST /_cluster/reroute
{
  "commands": [
    {
      "move": {
        "index": "my_index_name",
        "shard": 0,
        "from_node": "node1",
        "to_node": "node2"
      }
    },
    {
      "allocate": {
        "index": "my_index_name",
        "shard": 1,
        "node": "node3"
      }
    }
  ]
}

Updating settings

Change dynamically the minimum number of nodes to allow a master election, both persistent or not:

PUT /_cluster/settings
{
  "persistent": {
    "discovery.zen.minimum_master_nodes": 3
  }
}

PUT /_cluster/settings
{
  "transient": {
    "discovery.zen.minimum_master_nodes": 2
  }
}

Disable shard allocation, useful before a rolling restart:

PUT /_cluster/settings
{
    "transient" : {
        "cluster.routing.allocation.enable" : "none"
    }
}

PUT /_cluster/settings
{
    "transient" : {
        "cluster.routing.allocation.enable" : "all"
    }
}

Most useful plugins

Site plugins are no longer supported, look at Kibana applications or other standalone app like
Cerebro for basic management.

Kopf

Nice and easy user interface to manage Elasticsearch, with forms for a lots of API.

bin/plugin --install lmenezes/elasticsearch-kopf/1.0 bin/plugin install lmenezes/elasticsearch-kopf
/2.0
Marvel

Official monitoring solution.

bin/plugin --install elasticsearch/marvel/1.3
Elasticsearch-HQ

Monitoring, Management, and statistics analysis.

bin/plugin --install royrusso/elasticsearch-HQ/v1.0.0 bin/plugin install royrusso/elasticsearch-HQ
Mapper-Attachments

Index file attachments in over a thousand formats (such as PPT, XLS, PDF) using the Apache text
extraction library Tika.

bin/plugin --install elasticsearch/elasticsearch-mapper-attachments/2.7.1 bin/plugin install
elasticsearch/elasticsearch-mapper-attachments
Analysis ICU

Adding useful tokenizer and token filters from the Unicode ICU library.

bin/plugin --install elasticsearch/elasticsearch-analysis-icu/2.7.0 bin/plugin install analysis-icu
bin/elasticsearch-plugin install analysis-icu
AWS Cloud

Allow discovery and storage in Amazon cloud (EC2 and S3).

bin/plugin --install elasticsearch/elasticsearch-cloud-aws/2.7.1 bin/plugin install cloud-aws bin/
elasticsearch-plugin install discovery-ec2 bin/elasticsearch-plugin install repository-s3
Azure Cloud

Allow discovery and storage in Microsoft Azure cloud.

bin/plugin --install elasticsearch/elasticsearch-cloud-azure/2.8.3 bin/plugin install cloud-azure
bin/elasticsearch-plugin install discovery-azure-classic bin/elasticsearch-plugin install
repository-azure

Plugins management

bin/plugin --url file:///path/to/plugin --install plugin-name

bin/plugin --remove [pluginname]

bin/plugin install file:///path/to/plugin

bin/plugin list

bin/plugin remove [pluginname]

bin/elasticsearch-plugin install file:///path/to/plugin

bin/elasticsearch-plugin list

bin/elasticsearch-plugin remove [pluginname]

Other information

Where to find the plugin binary?

RPM: /usr/share/elasticsearch/bin

Debian: /usr/share/elasticsearch/bin

What are the default ports?

Kibana: http://localhost:5601/.

Elasticsearch: http://localhost:9200/.

How to set the correct HEAP SIZE value?

The best value for a single purpose Elasticsearch server is about 50% of available RAM but under
32g.
Assuming Ubuntu / Debian server, you can change those files:

/etc/security/limits.conf

elasticsearch - nofile 65535
elasticsearch - memlock unlimited

/etc/default/elasticsearch (on CentOS/RH: /etc/sysconfig/elasticsearch)

ES_HEAP_SIZE=20g
MAX_OPEN_FILES=65535
MAX_LOCKED_MEMORY=unlimited

Useful settings to change in elasticsearch.yml

cluster.name: jolicluster
node.name: ${HOSTNAME}
plugin.mandatory: marvel-agent,analysis-icu,license
node.data: true
node.master: true
bootstrap.mlockall: true
action.auto_create_index: +aaa*,-bbb*,+ccc*,-*

cluster.name: jolicluster
node.name: ${HOSTNAME}
discovery.zen.ping.unicast.hosts: ["front01", "front02"]
discovery.zen.minimum_master_nodes: 2
network.host: _site_
network.bind_host: [_site_, _local_]
plugin.mandatory: marvel-agent,analysis-icu,license
node.data: true
node.master: true
bootstrap.mlockall: true
action.auto_create_index: +aaa*,-bbb*,+ccc*,-*

Hosted on GitHub Pages. Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S.
and in other countries.

This website is not endorsed of affiliated with Elasticsearch. Brought to you by JoliCode.

#+end_example
** DONE elasticsearch DSL example
   CLOSED: [2016-10-01 Sat 23:07]
elasticdump \
--input=http://localhost:9200/.kibana \
--output=kibana-exported1.json \
--type=data \
--searchBody='{"filter": { "or": [ {"type": {"value": "search"}}, {"type": {"value": "dashboard"}}, {"type" : {"value":"visualization"}}] }}'

curl -XGET 'localhost:9200/.kibana/_search' | -d '
{
    "query" : {
        "match_all" : {"_type":"visualization"}
    }
}
' | python -m json.tool | less

curl -XPOST 'localhost:9200/.kibana' | -d '{
  "_type":"search"
}' | python -m json.tool | less
** TODO Use Marvel for elasticsearch
https://www.elastic.co/guide/en/elasticsearch/guide/current/_installing_elasticsearch.html

Marvel is a management and monitoring tool for Elasticsearch, which is
free for development use. It comes with an interactive console called
Sense, which makes it easy to talk to Elasticsearch directly from your
browser.
** DONE elasticsearch insert document
   CLOSED: [2016-10-02 Sun 00:11]
http://logz.io/learn/complete-guide-elk-stack/
curl -X POST http://127.0.0.1:9200/logs/my_app -d '{"timestamp": "2015-01-18 12:34:56", "message": "User logged in", "user_id": 4, "admin": false}'
 {
 "_id": "AVJWJkaW0D5QbnIxzP5S",
 "_index": "logs",
 "_shards": {
 "failed": 0,
 "successful": 1,
 "total": 2
 },
 "_type": "my_app",
 "_version": 1,
 "created": true
 }

&gt; curl -X PUT http://127.0.0.1:9200/app/users/4 -d '{"id": 4, "username": "john", "last_login": "2015-01-18 12:34:56"}'
 {
 "_id": "4",
 "_index": "app",
 "_shards": {
 "failed": 0,
 "successful": 1,
 "total": 2
 },
 "_type": "users",
 "_version": 1,
 "created": true
 }
** DONE elasticdump: elasticsearch import and export an index
   CLOSED: [2016-10-01 Sat 20:14]
https://github.com/taskrabbit/elasticsearch-dump

https://news.ycombinator.com/item?id=7779380
http://stackoverflow.com/questions/25144034/how-to-copy-some-elasticsearch-data-to-a-new-index
https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html
*** elasticdump tool
# export
elasticdump \
  --input=http://localhost:9200/.kibana \
  --output=kibana_schema.json \
  --type=mapping

elasticdump \
  --input=http://localhost:9200/.kibana \
  --output=kibana_data.json \
  --type=data

# import
elasticdump \
  --input=http://localhost:9200/.kibana \
  --output=kibana_data.json \
  --type=data
*** snapshot feature
    mkdir -p /opt/elasticsearch/snapshot && \
    chown elasticsearch:elasticsearch /opt/elasticsearch/snapshot && \
    echo "path.repo: [\"/opt/elasticsearch/snapshot\"]" >> /opt/elasticsearch/config/elasticsearch.yml
# restart elasticsearch

# create repository
curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{
    "type": "fs",
    "settings": {
        "compress": false,
        "location": "/opt/elasticsearch/snapshot"
    }
}'

# backup one index
curl -XPUT 'http://localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true' -d '{
    "type": "fs",
    "settings": {
        "location": "/opt/elasticsearch/snapshot",
        "indices": ".kibana",
        "compress": false
    }
}'

ls -lth /opt/elasticsearch/snapshot
*** DONE elasticdump: Cannot switch to old mode now: need to update nodejs
    CLOSED: [2016-10-03 Mon 08:44]
https://github.com/taskrabbit/elasticsearch-dump/issues/164
https://github.com/taskrabbit/elasticsearch-dump/issues/169
** DONE elasticsearch delete document by id
   CLOSED: [2016-10-04 Tue 21:09]
https://www.elastic.co/guide/en/elasticsearch/reference/2.3/_deleting_documents.html
Deleting Documentsedit
Deleting a document is fairly straightforward. This example shows how to delete our previous customer with the ID of 2:

curl -XDELETE 'localhost:9200/customer/external/2?pretty'
The delete-by-query plugin can delete all documents matching a specific query.
** DONE es index.query.bool.max_clause_count: curl "172.18.0.3:9200/_settings?pretty" | grep max_clause_count
   CLOSED: [2016-10-20 Thu 17:21]
** DONE elasticsearch get shards count and replica count
   CLOSED: [2016-11-15 Tue 12:41]
curl 173.255.243.91:9200/_settings?pretty

curl 173.255.243.91:9200/master-index-463da170a11811e6b9ecf23c912c9525/_settings?pretty
#+BEGIN_EXAMPLE
root@explorees1:~# curl 173.255.243.91:9200/_settings?pretty
curl 173.255.243.91:9200/_settings?pretty
{
  "master-index-463da170a11811e6b9ecf23c912c9525" : {
    "settings" : {
      "index" : {
        "path" : {
          "data" : "/data/elasticsearch/mdm/master/data",
          "logs" : "/data/elasticsearch/mdm/master/logs",
          "plugins" : "/data/elasticsearch/plugins",
          "work" : "/data/elasticsearch/mdm/master/work"
        },
        "mapping" : {
          "nested_fields" : {
            "limit" : "512"
          }
        },
        "number_of_shards" : "10",
        "query" : {
          "bool" : {
            "max_clause_count" : "10240"
          }
        },
        "creation_date" : "1478103633946",
        "analysis" : {
          "analyzer" : {
            "folding" : {
              "filter" : [ "lowercase", "asciifolding" ],
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "PNjlApM9To-ykH-CRa5uCQ",
        "version" : {
          "created" : "2030399"
        }
      }
    }
  },
  "staging-index-463da170a11811e6b9ecf23c912c9525" : {
    "settings" : {
      "index" : {
        "path" : {
          "data" : "/data/elasticsearch/mdm/staging/data",
          "logs" : "/data/elasticsearch/mdm/staging/logs",
          "plugins" : "/data/elasticsearch/plugins",
          "work" : "/data/elasticsearch/mdm/staging/work"
        },
        "mapping" : {
          "nested_fields" : {
            "limit" : "512"
          }
        },
        "number_of_shards" : "10",
        "query" : {
          "bool" : {
            "max_clause_count" : "10240"
          }
        },
        "creation_date" : "1478103634319",
        "analysis" : {
          "analyzer" : {
            "folding" : {
              "filter" : [ "lowercase", "asciifolding" ],
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "I7osNHlkTFSc8jyGHBmjcw",
        "version" : {
          "created" : "2030399"
        }
      }
    }
  },
  "staging-index-1e10a500a1a211e6862df23c912c953c" : {
    "settings" : {
      "index" : {
        "path" : {
          "data" : "/data/elasticsearch/mdm/staging/data",
          "logs" : "/data/elasticsearch/mdm/staging/logs",
          "plugins" : "/data/elasticsearch/plugins",
          "work" : "/data/elasticsearch/mdm/staging/work"
        },
        "mapping" : {
          "nested_fields" : {
            "limit" : "512"
          }
        },
        "number_of_shards" : "10",
        "query" : {
          "bool" : {
            "max_clause_count" : "10240"
          }
        },
        "creation_date" : "1478162835451",
        "analysis" : {
          "analyzer" : {
            "folding" : {
              "filter" : [ "lowercase", "asciifolding" ],
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "cR3ocujrTpOOHUAjuixrwA",
        "version" : {
          "created" : "2030399"
        }
      }
    }
  },
  "staging-index-e4010da4110ba377d100f050cb4440db" : {
    "settings" : {
      "index" : {
        "path" : {
          "data" : "/data/elasticsearch/mdm/staging/data",
          "logs" : "/data/elasticsearch/mdm/staging/logs",
          "plugins" : "/data/elasticsearch/plugins",
          "work" : "/data/elasticsearch/mdm/staging/work"
        },
        "mapping" : {
          "nested_fields" : {
            "limit" : "512"
          }
        },
        "number_of_shards" : "10",
        "query" : {
          "bool" : {
            "max_clause_count" : "10240"
          }
        },
        "creation_date" : "1478067736342",
        "analysis" : {
          "analyzer" : {
            "folding" : {
              "filter" : [ "lowercase", "asciifolding" ],
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "ykaQZC1nRzSdBBJswN9_vg",
        "version" : {
          "created" : "2030399"
        }
      }
    }
  },
  "master-index-e4010da4110ba377d100f050cb4440db" : {
    "settings" : {
      "index" : {
        "path" : {
          "data" : "/data/elasticsearch/mdm/master/data",
          "logs" : "/data/elasticsearch/mdm/master/logs",
          "plugins" : "/data/elasticsearch/plugins",
          "work" : "/data/elasticsearch/mdm/master/work"
        },
        "mapping" : {
          "nested_fields" : {
            "limit" : "512"
          }
        },
        "number_of_shards" : "10",
        "query" : {
          "bool" : {
            "max_clause_count" : "10240"
          }
        },
        "creation_date" : "1478067736035",
        "analysis" : {
          "analyzer" : {
            "folding" : {
              "filter" : [ "lowercase", "asciifolding" ],
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "kdVxgbGWTIeZOYnolKpAcQ",
        "version" : {
          "created" : "2030399"
        }
      }
    }
  },
  "master-index-1e10a500a1a211e6862df23c912c953c" : {
    "settings" : {
      "index" : {
        "path" : {
          "data" : "/data/elasticsearch/mdm/master/data",
          "logs" : "/data/elasticsearch/mdm/master/logs",
          "plugins" : "/data/elasticsearch/plugins",
          "work" : "/data/elasticsearch/mdm/master/work"
        },
        "mapping" : {
          "nested_fields" : {
            "limit" : "512"
          }
        },
        "number_of_shards" : "10",
        "query" : {
          "bool" : {
            "max_clause_count" : "10240"
          }
        },
        "creation_date" : "1478162834905",
        "analysis" : {
          "analyzer" : {
            "folding" : {
              "filter" : [ "lowercase", "asciifolding" ],
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "LAejDmcQT06IqQO43fiVVA",
        "version" : {
          "created" : "2030399"
        }
      }
    }
  },
  "master-index-8cd6e43115e9416eb23609486fa053e3" : {
    "settings" : {
      "index" : {
        "path" : {
          "data" : "/data/elasticsearch/mdm/master/data",
          "logs" : "/data/elasticsearch/mdm/master/logs",
          "plugins" : "/data/elasticsearch/plugins",
          "work" : "/data/elasticsearch/mdm/master/work"
        },
        "mapping" : {
          "nested_fields" : {
            "limit" : "512"
          }
        },
        "number_of_shards" : "10",
        "query" : {
          "bool" : {
            "max_clause_count" : "10240"
          }
        },
        "creation_date" : "1478067698454",
        "analysis" : {
          "analyzer" : {
            "folding" : {
              "filter" : [ "lowercase", "asciifolding" ],
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "lmBWY9mISjK50qBqfLKksw",
        "version" : {
          "created" : "2030399"
        }
      }
    }
  },
  "staging-index-8cd6e43115e9416eb23609486fa053e3" : {
    "settings" : {
      "index" : {
        "path" : {
          "data" : "/data/elasticsearch/mdm/staging/data",
          "logs" : "/data/elasticsearch/mdm/staging/logs",
          "plugins" : "/data/elasticsearch/plugins",
          "work" : "/data/elasticsearch/mdm/staging/work"
        },
        "mapping" : {
          "nested_fields" : {
            "limit" : "512"
          }
        },
        "number_of_shards" : "10",
        "query" : {
          "bool" : {
            "max_clause_count" : "10240"
          }
        },
        "creation_date" : "1478067698867",
        "analysis" : {
          "analyzer" : {
            "folding" : {
              "filter" : [ "lowercase", "asciifolding" ],
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "V0WVenT2SF6c0XglV1iRCA",
        "version" : {
          "created" : "2030399"
        }
      }
    }
  }
}
#+END_EXAMPLE
** DONE ElasticSearch has a limitation that an index cannot be re-sharded without re-indexing
   CLOSED: [2016-11-15 Tue 15:20]
http://memaddr.com/elasticsearch-how-many-shards-replicas-should-an-index-have/
There's no definitive answer. If we have a 3 nodes cluster, should the number of shards less than 3, equals 3, or greater than 3? Less than 3 should be ruled out obviously since some nodes are gonna be idle. Is 3 shards a good idea? Probably not, ElasticSearch has a limitation that an index cannot be re-sharded without re-indexing, the 3 shards work fine in your 3 nodes cluster until you add a new node, now the index only has 3 shards, in order to use the new node, data must be re-indexed, this could be a nightmare if the data is huge.
** DONE verify cluster setting: for allocation status
   CLOSED: [2017-01-15 Sun 08:41]
curl $es_ip:9200/_cluster/settings?v
#+BEGIN_EXAMPLE
root@prod-es-16:/tmp# curl $es_ip:9200/_cluster/settings?v
{"persistent":{"action":{"destructive_requires_name":"true"},"cluster":{"routing":{"allocation":{"enable":"all"}}}},"transient":{}}
#+END_EXAMPLE
** #  --8<-------------------------- separator ------------------------>8--
** DONE scripts to backup/restore es snapshots from Kung
   CLOSED: [2017-02-01 Wed 16:04]
#+BEGIN_EXAMPLE
# backup elasticsearch
vi /etc/elasticsearch/elasticsearch.yml
path.repo: ["/tmp/backups"]
# create repo
curl -XPUT 'http://172.17.0.3:9200/_snapshot/backups' -d '{
    "type": "fs",
    "settings": {
        "location": "/tmp/backups",
        "compress": true
    }
}'
# get back to verify
curl -XGET 'http://172.17.0.3:9200/_snapshot/backups'
# take snapshot
curl -XPUT 'http://172.17.0.3:9200/_snapshot/backups/snapshot_201013_1?wait_for_completion=true'
#+END_EXAMPLE
** #  --8<-------------------------- separator ------------------------>8--
** DONE elasticsearch cluster disk usage: curl $es_ip:9200/_cluster/stats?human&pretty
   CLOSED: [2017-01-30 Mon 14:56]
** DONE [Command] Change replica count of all es indices
   CLOSED: [2017-04-25 Tue 10:32]
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9200

curl $es_ip:$es_port/_cat/indices?v

# change replica count for all indices
for index in $(curl $es_ip:$es_port/_cat/indices?v | grep open | awk -F' ' '{print $3}'); do
     echo "Enable replica as 0 for $index"
     curl -XPUT "$es_ip:$es_port/$index/_settings" -d '
     {
         "index" : {
             "number_of_replicas": 0
         }
     }'
done

# verify status
curl $es_ip:$es_port/_cat/indices?v
** DONE [Command] ES Change replica count
   CLOSED: [2017-02-07 Tue 22:22]
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html

scp -P 2702 -r -i /tmp/copy_id_rsa root@138.197.217.168://data/elasticsearch/repo/mdm_backup_one_index /tmp/

es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
echo $es_ip

tail /etc/elasticsearch/elasticsearch.yml

curl -XPUT "$es_ip:9200/staging-index-d3ae95b0d12811e69edf0401f8d88501/_settings" -d '
{
    "index" : {
        "number_of_replicas" : 5
    }
}'
** DONE [Command] elasticsearch close index
   CLOSED: [2017-02-06 Mon 16:10]
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-open-close.html
POST /my_index/_close
POST /my_index/_open

curl -XPOST "http://$es_ip:9200/my_index/_close"
curl -XPOST "http://$es_ip:9200/my_index/_open"
** DONE elasticsearch delete index: curl -XDELETE "http://$es_ip:9200/master-index-e4010da4110ba377d100f050cb4440db"
   CLOSED: [2017-02-06 Mon 15:08]
** DONE Due to network partition, elasticsearch runs into single-node mode
   CLOSED: [2017-02-16 Thu 15:19]
[2017-02-16 16:06:57,337][INFO ][cluster.service          ] [prod-es-19] removed {{prod-es-24}{AATDvt0RRc6LXeJZX3nwCg}{138.197.217.103}{138.197.217.103:9300}{max_local_storage_nodes=1},}, reason: zen-disco-receive(from master [{prod-es-20}{1MrKLMdIShC-vKG23dBPJw}{138.68.46.207}{138.68.46.207:9300}{max_local_storage_nodes=1}])

prod-es-18(138.197.217.98)
prod-es-22(138.197.198.250)
prod-es-24(138.197.217.103)

prod-cb-07(138.197.217.56)

For ES cluster, can't reach 3 nodes.

> [2017-02-16 16:06:57,337][INFO ][cluster.service          ] [prod-es-19] removed {{prod-es-24}{AATDvt0RRc6LXeJZX3nwCg}{138.197.217.103}{138.197.217.103:9300}{max_local_storage_nodes=1},}, reason: zen-disco-receive(from master [{prod-es-20}{1MrKLMdIShC-vKG23dBPJw}{138.68.46.207}{138.68.46.207:9300}{max_local_storage_nodes=1}])
> [2017-02-16 16:07:27,754][INFO ][cluster.service          ] [prod-es-19] removed {{prod-es-18}{V_4QjsfLSRi2AhKzfS-x0w}{138.197.217.98}{138.197.217.98:9300}{max_local_storage_nodes=1},}, reason: zen-disco-receive(from master [{prod-es-20}{1MrKLMdIShC-vKG23dBPJw}{138.68.46.207}{138.68.46.207:9300}{max_local_storage_nodes=1}])
> [2017-02-16 16:07:58,182][INFO ][cluster.service          ] [prod-es-19] removed {{prod-es-22}{W_oornAwQI-h43HxFCfAvA}{138.197.198.250}{138.197.198.250:9300}{max_local_storage_nodes=1},}, reason: zen-disco-receive(from master [{prod-es-20}{1MrKLMdIShC-vKG23dBPJw}{138.68.46.207}{138.68.46.207:9300}{max_local_storage_nodes=1}])
** DONE elasticssearch delete remove all indices
   CLOSED: [2017-02-09 Thu 10:13]
http://elasticsearch-users.115913.n3.nabble.com/Delete-all-Indices-through-curl-API-td4046999.html

es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

for index in $(curl $es_ip:9200/_cat/shards?v | grep " p " | awk -F" " '{print $1}' | sort | uniq); do
     echo "curl -XDELETE http://$es_ip:9200/$index"
     curl -XDELETE "http://$es_ip:9200/$index"
done

# verify shards after deletion
curl $es_ip:9200/_cat/shards?v
** DONE list elasticsearch plugins: /usr/share/elasticsearch/bin/plugin list
   CLOSED: [2017-02-28 Tue 17:26]
https://www.elastic.co/guide/en/elasticsearch/plugins/current/listing-removing.html
/usr/share/elasticsearch/bin/plugin list | grep head
** DONE We can change ES replica count, without ES reindex
   CLOSED: [2017-03-27 Mon 12:53]
** DONE [#A] Fail to re-index, since some primary shards of new index is not available
   CLOSED: [2017-03-29 Wed 13:43]
#+BEGIN_EXAMPLE
100  2400  100  2168  100   232     35      3  0:01:17  0:01:00  0:00:17   568
{
  "took" : 60635,
  "timed_out" : false,
  "total" : 23599417,
  "updated" : 0,
  "created" : 0,
  "batches" : 1,
  "version_conflicts" : 96,
  "noops" : 0,
  "retries" : 0,
  "failures" : [ {
    "index" : "staging-index-839920f07e6b11e6b71d0401f8d88101-new",
    "type" : "b051ecd0b66511e69eed0401f8d88501_cnb000",
    "id" : "a65eb1c4a0208fb594df4f49faaa95d2",
    "cause" : {
      "type" : "unavailable_shards_exception",
      "reason" : "[staging-index-839920f07e6b11e6b71d0401f8d88101-new][8] primary shard is not active Timeout: [1m], request: [BulkShardRequest to [staging-index-839920f07e6b11e
6b71d0401f8d88101-new] containing [4] requests]"
    },
    "status" : 503
  }, {
    "index" : "staging-index-839920f07e6b11e6b71d0401f8d88101-new",
    "type" : "b051ecd0b66511e69eed0401f8d88501_cnb000",
    "id" : "08f5ba1b0d0d6aa578f17bc9ca6b1462",
    "cause" : {
      "type" : "unavailable_shards_exception",
      "reason" : "[staging-index-839920f07e6b11e6b71d0401f8d88101-new][8] primary shard is not active Timeout: [1m], request: [BulkShardRequest to [staging-index-839920f07e6b11e
6b71d0401f8d88101-new] containing [4] requests]"
    },
    "status" : 503
  }, {
    "index" : "staging-index-839920f07e6b11e6b71d0401f8d88101-new",
    "type" : "b051ecd0b66511e69eed0401f8d88501_cnb000",
    "id" : "0a8dd16504821100482fada064f0cfbb",
    "cause" : {
      "type" : "unavailable_shards_exception",
      "reason" : "[staging-index-839920f07e6b11e6b71d0401f8d88101-new][8] primary shard is not active Timeout: [1m], request: [BulkShardRequest to [staging-index-839920f07e6b11e
6b71d0401f8d88101-new] containing [4] requests]"
    },
#+END_EXAMPLE

#+BEGIN_EXAMPLE
Every 2.0s: curl 10.1.1.12:9200/_cat/shards?v | grep -v STARTED                                                                                          Wed Mar 29 15:57:04 2017

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 100 70040  100 70040    0     0   237k      0 --:--:-- --:--:-- --:--:--  238k
index                                              shard prirep state           docs   store ip        node
staging-index-8cd6e43115e9416eb23609486fa053e3     3     r      RELOCATING  19341673  23.2gb 10.1.1.39 bematech-es-14 -> 10.1.1.18 PIvm3FM2TiKB0rQdT321Kg bematech-es-7
staging-index-839920f07e6b11e6b71d0401f8d88101     0     r      RELOCATING  23668454    73gb 10.1.1.46 bematech-es-17 -> 10.1.1.28 Hxh2DyA-QmG1bsVFMnIASA bematech-es-10
staging-index-839920f07e6b11e6b71d0401f8d88101-new 8     p      UNASSIGNED
staging-index-839920f07e6b11e6b71d0401f8d88101-new 8     r      UNASSIGNED
staging-index-839920f07e6b11e6b71d0401f8d88101-new 8     r      UNASSIGNED
#+END_EXAMPLE
** DONE ES alias points to multiple ES indices
   CLOSED: [2017-03-28 Tue 18:07]
http://stackoverflow.com/questions/19661822/is-it-possible-to-write-to-multiple-indexes-with-an-elasticsearch-alias
An alias can also be mapped to more than one index, and when specifying it, the alias will automatically expand to the aliases indices.

#+BEGIN_EXAMPLE
Denny Zhang [5:58 PM]
Let's say we have one alias points to multiple ES indices.

If we write the the alias, would it be write to all ES indices, or it just choose one?

[5:59]
http://stackoverflow.com/questions/19661822/is-it-possible-to-write-to-multiple-indexes-with-an-elasticsearch-alias
stackoverflow.com
Is it possible to write to multiple indexes with an ElasticSearch alias?
The ElasticSearch Docs reads: An alias can also be mapped to more than one index, and when specifying it, the alias will automatically expand to the aliases indices. But when I try to add an al...

Denny Zhang [6:01 PM]
Also it seems writing will fail like below
```ElasticSearchIllegalArgumentException[Alias [dev_write] has more than one indices associated with it [[dev_01, dev]], can't execute a single index op```
#+END_EXAMPLE
** DONE change ES alias command
   CLOSED: [2017-03-29 Wed 13:45]
curl -XPOST "http://prod-es-16:9200/_aliases" -d '{
"actions" : [
{ "add" : { "index" : "master-index-860245c0841e11e6a8260401f8d88101-new", "alias" : "master-860245c0841e11e6a8260401f8d88101" } },
{ "add" : { "index" : "master-index-799e458055c611e6bb000401f8d88101-new", "alias" : "master-799e458055c611e6bb000401f8d88101" } },
{ "remove" : { "index" : "master-index-860245c0841e11e6a8260401f8d88101-new", "alias" : "master-799e458055c611e6bb000401f8d88101" } }     ] }'

curl -XPOST "http://${es_ip}:${es_port}/master-index-860245c0841e11e6a8260401f8d88101/_close"
** DONE elasticsearch list aliases: curl -XGET "http://${es_ip}:9200/_aliases?pretty"
   CLOSED: [2017-03-27 Mon 13:36]
** check es alias
curl -XGET "http://$es_ip:9200/_aliases?pretty"
** DONE change back alias
   CLOSED: [2017-03-29 Wed 15:20]

# remove alias
curl -XPOST "http://$es_ip:9200/_aliases" -d '{
"actions" : [
{"add":{"index":"staging-index-839920f07e6b11e6b71d0401f8d88101","alias":"staging-839920f07e6b11e6b71d0401f8d88101"}},
{"remove":{"index":"staging-index-839920f07e6b11e6b71d0401f8d88101-new","alias":"staging-839920f07e6b11e6b71d0401f8d88101"}}]}'

# check alias
curl -XGET "http://${es_ip}:9200/_aliases?pretty" | grep staging-index-839920f07e6b11e6b71d0401f8d88101 -C 3

# list index
curl $es_ip:9200/_cat/indices?v | grep staging-index-839920f07e6b11e6b71d0401f8d88101

# close index
curl -XPOST "http://${es_ip}:9200/staging-index-839920f07e6b11e6b71d0401f8d88101-new/_close
** DONE GET index setting: curl -XGET "http://${es_ip}:${es_port}/${new_index_name}/_settings?pretty"
   CLOSED: [2017-03-30 Thu 07:44]
** DONE ES indices api headers: (health status index pri rep docs.count docs.deleted store.size pri.store.size)
   CLOSED: [2017-03-30 Thu 00:05]
health status index                                              pri rep docs.count docs.deleted store.size pri.store.size
green  open   messagebroker                                        5   2          0            0      2.3kb           795b
green  open   flex2gateway                                         5   2          0            0      2.3kb           795b
** DONE elasticsearch check index exists: HEAD $INDEX, then check return code
   CLOSED: [2017-03-29 Wed 19:28]
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-exists.html
** DONE improve the performance of reindex batch size
   CLOSED: [2017-03-30 Thu 09:40]
https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-reindex.html
By default _reindex uses scroll batches of 100. You can change the batch size with the size field in the source element:

POST /_reindex
{
  "source": {
    "index": "source",
    "size": 1500
  },
  "dest": {
    "index": "dest"
  }
}
** DONE why giant ES index is bad
   CLOSED: [2017-03-30 Thu 10:57]
Kung Wang [10:44 AM]
so @denny.zhang, if we do move back to DO, what machine you suggest we should use? still 32G one or 64G one for elasticsearch?

Denny Zhang [10:45 AM]
I think 32GB should be good enough.

Since we will have 50 shards for that giant index.

[10:46]
That means which shard will have 20GB, considering to current data volume.

[10:48]
=================
BTW, we need to re-think whether we can avoid giant ES index from design level.

I know I have mentioned this before. *Just raise again: keep ES index no bigger than 30GB seems to be a good practice*

Indices with TB data is really bad, from our experience.
Any change we try to make, it will take hours or even days. Not to mention those OOM issues.

Copy @bruno & @robson.poffo (edited)

Bruno Volpato [10:50 AM]
Nothing we can really break now... our platform would need to support that

[10:50]
But oom are due to big shards, not indexes right?

Denny Zhang [10:52 AM]
Big shards are problems. It should be where the OOM come from.

But big indices are also bad in following cases:

1. Search at index level.
   Especially when we have cut the indices into 50+ pieces.
   That mean complex searches will involves almost all ES nodes each time.
2. Backup, copy or re-index at index level
   Any actions will take days for indices at TB level.

Right? (edited)

[10:53]
Progress: 80%
** DONE check status of ES data deletion
   CLOSED: [2017-03-30 Thu 16:57]
curl "http://azure-es-01.carol.ai:9200/*/mdmreceiptMasterHistory,mdmaiproductMasterHistory,333584e0b3a511e698870401f8d88101_receitaws,333584e0b3a511e698870401f8d88101_receitafederal/_count"

watch "curl \"http://$es_ip:9200/*/mdmreceiptMasterHistory,mdmaiproductMasterHistory,333584e0b3a511e698870401f8d88101_receitaws,333584e0b3a511e698870401f8d88101_receitafederal/_count\""

date

curl "http://$es_ip:9200/*/mdmreceiptMasterHistory,mdmaiproductMasterHistory,333584e0b3a511e698870401f8d88101_receitaws,333584e0b3a511e698870401f8d88101_receitafederal/_count"

date
sleep 1

curl "http://$es_ip:9200/*/mdmreceiptMasterHistory,mdmaiproductMasterHistory,333584e0b3a511e698870401f8d88101_receitaws,333584e0b3a511e698870401f8d88101_receitafederal/_count"

date
** DONE ES re-index requirement
   CLOSED: [2017-03-30 Thu 08:19]
Reindex does not attempt to set up the destination index. It does not copy the settings of the source index. You should set up the destination index prior to running a _reindex action, including setting up mappings, shard counts, replicas, etc.
** [#A] Manually commands to trigger re-index                     :IMPORTANT:
*** get index setting and mappings
time curl -XGET "http://${es_ip}:9200/MY_INDEX_CHANGE_THIS/_settings?pretty"

# Download index setting
curl "http://${es_ip}:${es_port}/${old_index_name}/_settings" | jq ".[] | .settings.index.number_of_shards=\"${shard_count}\" | .settings.index.number_of_replicas=\"${replica_count}\"" > "${tmp_dir}/settings.json"
curl "http://${es_ip}:${es_port}/${old_index_name}/_mapping" | jq '.[]' > "${tmp_dir}/mapping.json"
cat "${tmp_dir}/mapping.json" "${tmp_dir}/settings.json" | jq --slurp '.[0] * .[1]' > "${tmp_dir}/create.json"

# Create new index
time curl -XPOST "http://${es_ip}:${es_port}/${new_index_name}" -d @"${tmp_dir}/create.json"

# Check new index setting
curl -XGET "http://${es_ip}:9200/${new_index_name}/_settings?pretty"
*** re-index
time curl -XPOST "http://${es_ip}:9200/_reindex?pretty" -d "
    {
    \"conflicts\": \"proceed\",
    \"source\": {
    \"index\": \"staging-index-CHANGETHIS-new\",
    \"size\": \"1000\"
    },
    \"dest\": {
    \"index\": \"staging-index-CHANGETHIS-new2\",
    \"op_type\": \"create\"
    }
}" | tee -a "/var/log/es_reindex_manual.log"
*** check status
curl $es_ip:9200/_cat/shards?v | grep -v STARTED
curl $es_ip:9200/_cat/nodes?v
curl $es_ip:9200/_cluster/health?pretty
*** update alias
time curl -XPOST "http://${es_ip}:9200/_aliases" -d "
{
    \"actions\": [
    { \"remove\": {
    \"alias\": \"staging-CHANGETHIS\",
    \"index\": \"staging-index-CHANGETHIS-old\"
    }},
    { \"add\": {
    \"alias\": \"staging-CHANGETHIS\",
    \"index\": \"staging-index-CHANGETHIS-new\"
    }}
    ]
}" | tee -a "/var/log/es_reindex_manual.log"
*** check alias status
curl -XGET "http://${es_ip}:9200/_aliases?pretty" | grep -C 5 staging-CHANGETHIS

curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep -C 5 staging-index-CHANGETHIS
*** close/delete index
my_index="audit-abae8b30ac9b11e692000401f8d88101-new"
curl $es_ip:$es_port/_cat/indices?v | grep $my_index
curl -XPOST "http://$es_ip:$es_port/$my_index/_close"
curl $es_ip:$es_port/_cat/indices?v | grep $my_index

my_index="audit-abae8b30ac9b11e692000401f8d88101-new"
curl $es_ip:$es_port/_cat/indices?v | grep $my_index
curl -XDELETE "http://$es_ip:$es_port/$my_index"
curl $es_ip:$es_port/_cat/indices?v | grep $my_index
** close all es indices
for index in $(curl $es_ip:9200/_cat/indices?v | grep "open" | awk -F" " '{print $3}'); do
     echo "curl -XPOST http://$es_ip:9200/$index/_close"
     curl -XPOST "http://$es_ip:9200/$index/_close"
done
** DONE ES 2.3 threadpool.search.queue_size
   CLOSED: [2017-06-02 Fri 22:41]
https://stackoverflow.com/questions/33110310/increasing-the-size-of-the-queue-in-elasticsearch

es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

curl -XGET  $es_ip:9400/_cluster/settings

curl -XGET  $es_ip:9400/_cluster/settings | grep threadpool

curl -XPUT  $es_ip:9400/_cluster/settings -d '{
    "persistent" : {
        "threadpool.search.queue_size" : 1000
** DONE ES create index timeout
   CLOSED: [2017-06-17 Sat 13:27]
curl $es_ip:9200/_cat/indices?v  | grep master-index-8a18aa800e5911e785f24a8136534b63
curl -XPOST "http://$es_ip:9200/master-index-8a18aa800e5911e785f24a8136534b63-new3/_close"
curl -XDELETE "http://$es_ip:9200/master-index-8a18aa800e5911e785f24a8136534b63-new3"

date
curl -XPUT "http://138.197.219.98:9200/master-index-8a18aa800e5911e785f24a8136534b63-test?timeout=10m&wait_for_active_shards=all" -d @./master-index-8a18aa800e5911e785f24a8136534b63_SettingsAndMappings.json
date
** DONE ES check whether alias exists
   CLOSED: [2017-06-18 Sun 21:34]
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html
curl -I "http://${es_ip}:${es_port}/_alias/master-098f6bcd4621d373cade4e832627b4f6"
** TODO Verify ES disable _all field
https://www.elastic.co/guide/en/elasticsearch/reference/2.3/mapping-all-field.html#disabling-all-field
** misc                                                            :noexport:
*** DONE wrong usage of es include._ip and exclude._ip
    CLOSED: [2017-06-20 Tue 23:59]
 curl -XGET $es_ip:9200/_cluster/settings
 curl -XPUT $es_ip:9200/_cluster/settings -d '{
   "transient" :{
       "cluster.routing.allocation.include._ip" : "138.197.219.98"
    }
 }'
**** DONE get ES setting
     CLOSED: [2017-06-20 Tue 23:58]
 curl http://prod-es-16:9200/_cluster/settings?pretty

 curl -XPUT $es_ip:9200/_cluster/settings -d '{
   "transient" :{
       "cluster.routing.allocation.exclude._ip" : ""
    }
 }'
**** DONE ES exclude api error
     CLOSED: [2017-06-20 Tue 21:37]
 root@prod-es-01:/var/log/elasticsearch# curl -XPUT $es_ip:9200/_cluster/settings -d '{
 >   "transient" :{
 >       "cluster.routing.allocation.exclude._ip" : ""
 >    }
 > }'



 {"error":{"root_cause":[{"type":"process_cluster_event_timeout_exception","reason":"failed to process cluster event (reroute_after_cluster_update_settings) within 30s"}],"type":"exception","reason":"reroute after update settings failed","caused_by":{"type":"process_cluster_event_timeout_exception","reason":"failed to process cluster event (reroute_after_cluster_update_settings) within 30s"}},"status":500}root@prod-es-01:/var/log/elasticsearch#
 root@prod-es-01:/var/log/elasticsearch#
 root@prod-es-01:/var/log/elasticsearch#
 root@prod-es-01:/var/log/elasticsearch# curl -XGET $es_ip:9200/_cluster/settings
 {"persistent":{"action":{"destructive_requires_name":"true"},"cluster":{"routing":{"allocation":{"enable":"all"}}}},"transient":{"cluster":{"routing":{"allocation":{"include":{"_ip":"*"},"exclude":{"_ip":""}}}}}}root@prod-es-01:/var/log/elasticsearch# curl -XPUT $es_ip:9200/_cluster/settings -d '{
 >   "transient" :{
 >       "cluster.routing.allocation.exclude._ip" : ""
 >    }
 > }'

 {"acknowledged":true,"persistent":{},"transient":{"cluster":{"routing":{"allocation":{"exclude":{"_ip":""}}}}}}root@prod-es-01:/var/log/elasticsearch#
*** DONE check when index has been created
    CLOSED: [2017-06-30 Fri 22:53]

 #+BEGIN_EXAMPLE
 >>> import datetime
 >>> s = 1236472051807 / 1000.0
 >>> datetime.datetime.fromtimestamp(s).strftime('%Y-%m-%d %H:%M:%S.%f')
 '2009-03-08 09:27:31.807000'
 #+END_EXAMPLE

 curl -XPOST "http://$es_ip:9400/_aliases" | grep  40e0def05c1f11e7a9954a8136534b63

 #+BEGIN_EXAMPLE
 root@bematech-do-es-1:~# curl -XGET "http://${es_ip}:9200/.kibana/_settings?pretty"
 {
   ".kibana" : {
     "settings" : {
       "index" : {
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1073741824",
         "creation_date" : "1472248049032",
         "number_of_replicas" : "2",
         "uuid" : "50_HcbrQQSSuO0ZA7J_2Gw",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   }
 }
 #+END_EXAMPLE
*** DONE [#A] ES get setting
    CLOSED: [2017-07-06 Thu 15:13]
 curl -XGET  $es_ip:9200/_cluster/settings
 curl -XGET  $es_ip:9200/_all/_settings?pretty
 curl -XGET "http://$es_ip:9200/_nodes/stats?pretty"
**** curl -XGET  $es_ip:9200/_cluster/settings
 #+BEGIN_EXAMPLE
 root@all-in-one-DockerDeployAllInOne-7:/# curl -XGET  $es_ip:9200/_cluster/settings
 curl -XGET  $es_ip:9200/_cluster/settings
 {"persistent":{},"transient":{}}
 #+END_EXAMPLE
**** curl -XGET  $es_ip:9200/_all/_settings?pretty
 #+BEGIN_EXAMPLE
 root@all-in-one-DockerDeployAllInOne-7:/# curl -XGET  $es_ip:9200/_all/_settings?pretty
 {
   "master-index-098f6bcd4621d373cade4e832627b4f6" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/master/data",
           "logs" : "/data/elasticsearch/mdm/master/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/master/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499368121603",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "ifnXQaf_TomqAaTMb-FdBw",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   },
   "staging-index-13a1f8adbec032ed68f3d035449ef48d" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/staging/data",
           "logs" : "/data/elasticsearch/mdm/staging/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/staging/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499368119032",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "DXLOnONRR1unanZpJkx9QA",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   },
   "master-index-13a1f8adbec032ed68f3d035449ef48d" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/master/data",
           "logs" : "/data/elasticsearch/mdm/master/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/master/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499368118919",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "Sjv0-r9TRdiuGvEMS25jXA",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   },
   "master-index-46078234297e400a1648d9c427dc8c4b" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/master/data",
           "logs" : "/data/elasticsearch/mdm/master/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/master/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499368069837",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "LQh5W9XVT5KdoG6EYB5LhQ",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   },
   "staging-index-098f6bcd4621d373cade4e832627b4f6" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/staging/data",
           "logs" : "/data/elasticsearch/mdm/staging/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/staging/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499368121680",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "6iwEUhJBR_GgRxpvolXHDg",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   },
   "staging-index-e4010da4110ba377d100f050cb4440db" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/staging/data",
           "logs" : "/data/elasticsearch/mdm/staging/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/staging/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499367886833",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "LO7u6DZrRBW0MgxYL7QIfw",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   },
   "staging-index-46078234297e400a1648d9c427dc8c4b" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/staging/data",
           "logs" : "/data/elasticsearch/mdm/staging/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/staging/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499368069946",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "X6dT47k8QGeIrBpyFVnppA",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   },
   "master-index-e4010da4110ba377d100f050cb4440db" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/master/data",
           "logs" : "/data/elasticsearch/mdm/master/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/master/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499367886747",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "A-yGdeE8QDa3VwKcuQe3oA",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   },
   "master-index-8cd6e43115e9416eb23609486fa053e3" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/master/data",
           "logs" : "/data/elasticsearch/mdm/master/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/master/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499367841927",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "bqFVD5SSSFijRkmpu4-vAw",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   },
   "staging-index-8cd6e43115e9416eb23609486fa053e3" : {
     "settings" : {
       "index" : {
         "path" : {
           "data" : "/data/elasticsearch/mdm/staging/data",
           "logs" : "/data/elasticsearch/mdm/staging/logs",
           "plugins" : "/data/elasticsearch/plugins",
           "work" : "/data/elasticsearch/mdm/staging/work"
         },
         "mapping" : {
           "nested_fields" : {
             "limit" : "512"
           }
         },
         "number_of_shards" : "1",
         "query" : {
           "bool" : {
             "max_clause_count" : "10240"
           }
         },
         "max_result_window" : "1048576",
         "creation_date" : "1499367842765",
         "analysis" : {
           "analyzer" : {
             "folding" : {
               "filter" : [ "lowercase", "asciifolding" ],
               "tokenizer" : "standard"
             }
           }
         },
         "number_of_replicas" : "0",
         "uuid" : "_FvxEFiyT7aeaLWFVFHG3w",
         "version" : {
           "created" : "2030399"
         }
       }
     }
   }
 }
 #+END_EXAMPLE
**** curl -XGET "http://$es_ip:9200/_nodes/stats?pretty"
 #+BEGIN_EXAMPLE
 root@all-in-one-DockerDeployAllInOne-7:/# curl -XGET "http://$es_ip:9200/_nodes/stats?pretty"
 {
   "cluster_name" : "totvs_DockerDeployAllInOne_7es",
   "nodes" : {
     "jLKPxkN7Qou4X1Sfkpt9BQ" : {
       "timestamp" : 1499371019764,
       "name" : "default-ubuntu-1404",
       "transport_address" : "all-in-one-DockerDeployAllInOne-7/172.17.0.3:9300",
       "host" : "172.17.0.3",
       "ip" : [ "all-in-one-DockerDeployAllInOne-7/172.17.0.3:9300", "NONE" ],
       "attributes" : {
         "max_local_storage_nodes" : "1"
       },
       "indices" : {
         "docs" : {
           "count" : 166053,
           "deleted" : 7
         },
         "store" : {
           "size_in_bytes" : 188835272,
           "throttle_time_in_millis" : 0
         },
         "indexing" : {
           "index_total" : 26,
           "index_time_in_millis" : 179,
           "index_current" : 0,
           "index_failed" : 0,
           "delete_total" : 0,
           "delete_time_in_millis" : 0,
           "delete_current" : 0,
           "noop_update_total" : 0,
           "is_throttled" : false,
           "throttle_time_in_millis" : 0
         },
         "get" : {
           "total" : 0,
           "time_in_millis" : 0,
           "exists_total" : 0,
           "exists_time_in_millis" : 0,
           "missing_total" : 0,
           "missing_time_in_millis" : 0,
           "current" : 0
         },
         "search" : {
           "open_contexts" : 0,
           "query_total" : 11017,
           "query_time_in_millis" : 5924,
           "query_current" : 0,
           "fetch_total" : 2007,
           "fetch_time_in_millis" : 882,
           "fetch_current" : 0,
           "scroll_total" : 11017,
           "scroll_time_in_millis" : 36972,
           "scroll_current" : 0
         },
         "merges" : {
           "current" : 0,
           "current_docs" : 0,
           "current_size_in_bytes" : 0,
           "total" : 0,
           "total_time_in_millis" : 0,
           "total_docs" : 0,
           "total_size_in_bytes" : 0,
           "total_stopped_time_in_millis" : 0,
           "total_throttled_time_in_millis" : 0,
           "total_auto_throttle_in_bytes" : 209715200
         },
         "refresh" : {
           "total" : 22,
           "total_time_in_millis" : 524
         },
         "flush" : {
           "total" : 9,
           "total_time_in_millis" : 23
         },
         "warmer" : {
           "current" : 0,
           "total" : 55,
           "total_time_in_millis" : 345
         },
         "query_cache" : {
           "memory_size_in_bytes" : 560,
           "total_count" : 141610,
           "hit_count" : 2509,
           "miss_count" : 139101,
           "cache_size" : 35,
           "cache_count" : 35,
           "evictions" : 0
         },
         "fielddata" : {
           "memory_size_in_bytes" : 508980,
           "evictions" : 0
         },
         "percolate" : {
           "total" : 0,
           "time_in_millis" : 0,
           "current" : 0,
           "memory_size_in_bytes" : -1,
           "memory_size" : "-1b",
           "queries" : 0
         },
         "completion" : {
           "size_in_bytes" : 0
         },
         "segments" : {
           "count" : 31,
           "memory_in_bytes" : 2478341,
           "terms_memory_in_bytes" : 2154145,
           "stored_fields_memory_in_bytes" : 49024,
           "term_vectors_memory_in_bytes" : 0,
           "norms_memory_in_bytes" : 8960,
           "doc_values_memory_in_bytes" : 266212,
           "index_writer_memory_in_bytes" : 0,
           "index_writer_max_memory_in_bytes" : 56505958,
           "version_map_memory_in_bytes" : 0,
           "fixed_bit_set_memory_in_bytes" : 15240
         },
         "translog" : {
           "operations" : 26,
           "size_in_bytes" : 11999
         },
         "suggest" : {
           "total" : 0,
           "time_in_millis" : 0,
           "current" : 0
         },
         "request_cache" : {
           "memory_size_in_bytes" : 0,
           "evictions" : 0,
           "hit_count" : 0,
           "miss_count" : 0
         },
         "recovery" : {
           "current_as_source" : 0,
           "current_as_target" : 0,
           "throttle_time_in_millis" : 0
         }
       },
       "os" : {
         "timestamp" : 1499371019791,
         "cpu_percent" : 4,
         "load_average" : 0.35,
         "mem" : {
           "total_in_bytes" : 25281347584,
           "free_in_bytes" : 14878916608,
           "used_in_bytes" : 10402430976,
           "free_percent" : 59,
           "used_percent" : 41
         },
         "swap" : {
           "total_in_bytes" : 536866816,
           "free_in_bytes" : 469614592,
           "used_in_bytes" : 67252224
         }
       },
       "process" : {
         "timestamp" : 1499371019791,
         "open_file_descriptors" : 347,
         "max_file_descriptors" : 64000,
         "cpu" : {
           "percent" : 0,
           "total_in_millis" : 114590
         },
         "mem" : {
           "total_virtual_in_bytes" : 6415073280
         }
       },
       "jvm" : {
         "timestamp" : 1499371019792,
         "uptime_in_millis" : 1531168,
         "mem" : {
           "heap_used_in_bytes" : 72453872,
           "heap_used_percent" : 13,
           "heap_committed_in_bytes" : 518979584,
           "heap_max_in_bytes" : 518979584,
           "non_heap_used_in_bytes" : 71524712,
           "non_heap_committed_in_bytes" : 72908800,
           "pools" : {
             "young" : {
               "used_in_bytes" : 6878840,
               "max_in_bytes" : 143130624,
               "peak_used_in_bytes" : 143130624,
               "peak_max_in_bytes" : 143130624
             },
             "survivor" : {
               "used_in_bytes" : 739008,
               "max_in_bytes" : 17891328,
               "peak_used_in_bytes" : 17891328,
               "peak_max_in_bytes" : 17891328
             },
             "old" : {
               "used_in_bytes" : 64836024,
               "max_in_bytes" : 357957632,
               "peak_used_in_bytes" : 64836024,
               "peak_max_in_bytes" : 357957632
             }
           }
         },
         "threads" : {
           "count" : 98,
           "peak_count" : 101
         },
         "gc" : {
           "collectors" : {
             "young" : {
               "collection_count" : 45,
               "collection_time_in_millis" : 975
             },
             "old" : {
               "collection_count" : 1,
               "collection_time_in_millis" : 85
             }
           }
         },
         "buffer_pools" : {
           "direct" : {
             "count" : 126,
             "used_in_bytes" : 22038068,
             "total_capacity_in_bytes" : 22038068
           },
           "mapped" : {
             "count" : 16,
             "used_in_bytes" : 112832775,
             "total_capacity_in_bytes" : 112832775
           }
         },
         "classes" : {
           "current_loaded_count" : 8166,
           "total_loaded_count" : 8166,
           "total_unloaded_count" : 0
         }
       },
       "thread_pool" : {
         "bulk" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "fetch_shard_started" : {
           "threads" : 1,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 7,
           "completed" : 10
         },
         "fetch_shard_store" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "flush" : {
           "threads" : 1,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 4,
           "completed" : 18
         },
         "force_merge" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "generic" : {
           "threads" : 1,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 5,
           "completed" : 171
         },
         "get" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "index" : {
           "threads" : 8,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 8,
           "completed" : 26
         },
         "listener" : {
           "threads" : 4,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 4,
           "completed" : 17
         },
         "management" : {
           "threads" : 3,
           "queue" : 0,
           "active" : 1,
           "rejected" : 0,
           "largest" : 3,
           "completed" : 157
         },
         "percolate" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "refresh" : {
           "threads" : 2,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 2,
           "completed" : 22
         },
         "search" : {
           "threads" : 13,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 13,
           "completed" : 12019
         },
         "snapshot" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "suggest" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "warmer" : {
           "threads" : 4,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 4,
           "completed" : 54
         }
       },
       "fs" : {
         "timestamp" : 1499371019793,
         "total" : {
           "total_in_bytes" : 405687627776,
           "free_in_bytes" : 187115646976,
           "available_in_bytes" : 166509871104,
           "spins" : "true"
         },
         "data" : [ {
           "path" : "/usr/share/elasticsearch/totvs_DockerDeployAllInOne_7es/nodes/0",
           "mount" : "/ (overlay)",
           "type" : "overlay",
           "total_in_bytes" : 405687627776,
           "free_in_bytes" : 187115646976,
           "available_in_bytes" : 166509871104,
           "spins" : "true"
         } ]
       },
       "transport" : {
         "server_open" : 26,
         "rx_count" : 29172,
         "rx_size_in_bytes" : 10386274,
         "tx_count" : 29170,
         "tx_size_in_bytes" : 6957740
       },
       "http" : {
         "current_open" : 1,
         "total_opened" : 13
       },
       "breakers" : {
         "request" : {
           "limit_size_in_bytes" : 103795916,
           "limit_size" : "98.9mb",
           "estimated_size_in_bytes" : 0,
           "estimated_size" : "0b",
           "overhead" : 1.0,
           "tripped" : 0
         },
         "fielddata" : {
           "limit_size_in_bytes" : 155693875,
           "limit_size" : "148.4mb",
           "estimated_size_in_bytes" : 508980,
           "estimated_size" : "497kb",
           "overhead" : 1.03,
           "tripped" : 0
         },
         "parent" : {
           "limit_size_in_bytes" : 259489792,
           "limit_size" : "247.4mb",
           "estimated_size_in_bytes" : 508980,
           "estimated_size" : "497kb",
           "overhead" : 1.0,
           "tripped" : 0
         }
       },
       "script" : {
         "compilations" : 0,
         "cache_evictions" : 0
       }
     },
     "avDeejpoQum3JZn6Z3snrg" : {
       "timestamp" : 1499371019766,
       "name" : "all-in-one-DockerDeployAllInOne-7node_backup",
       "transport_address" : "172.17.0.3:9301",
       "host" : "172.17.0.3",
       "ip" : [ "172.17.0.3:9301", "NONE" ],
       "attributes" : {
         "data" : "false",
         "master" : "false"
       },
       "indices" : {
         "docs" : {
           "count" : 0,
           "deleted" : 0
         },
         "store" : {
           "size_in_bytes" : 0,
           "throttle_time_in_millis" : 0
         },
         "indexing" : {
           "index_total" : 0,
           "index_time_in_millis" : 0,
           "index_current" : 0,
           "index_failed" : 0,
           "delete_total" : 0,
           "delete_time_in_millis" : 0,
           "delete_current" : 0,
           "noop_update_total" : 0,
           "is_throttled" : false,
           "throttle_time_in_millis" : 0
         },
         "get" : {
           "total" : 0,
           "time_in_millis" : 0,
           "exists_total" : 0,
           "exists_time_in_millis" : 0,
           "missing_total" : 0,
           "missing_time_in_millis" : 0,
           "current" : 0
         },
         "search" : {
           "open_contexts" : 0,
           "query_total" : 0,
           "query_time_in_millis" : 0,
           "query_current" : 0,
           "fetch_total" : 0,
           "fetch_time_in_millis" : 0,
           "fetch_current" : 0,
           "scroll_total" : 0,
           "scroll_time_in_millis" : 0,
           "scroll_current" : 0
         },
         "merges" : {
           "current" : 0,
           "current_docs" : 0,
           "current_size_in_bytes" : 0,
           "total" : 0,
           "total_time_in_millis" : 0,
           "total_docs" : 0,
           "total_size_in_bytes" : 0,
           "total_stopped_time_in_millis" : 0,
           "total_throttled_time_in_millis" : 0,
           "total_auto_throttle_in_bytes" : 0
         },
         "refresh" : {
           "total" : 0,
           "total_time_in_millis" : 0
         },
         "flush" : {
           "total" : 0,
           "total_time_in_millis" : 0
         },
         "warmer" : {
           "current" : 0,
           "total" : 0,
           "total_time_in_millis" : 0
         },
         "query_cache" : {
           "memory_size_in_bytes" : 0,
           "total_count" : 0,
           "hit_count" : 0,
           "miss_count" : 0,
           "cache_size" : 0,
           "cache_count" : 0,
           "evictions" : 0
         },
         "fielddata" : {
           "memory_size_in_bytes" : 0,
           "evictions" : 0
         },
         "percolate" : {
           "total" : 0,
           "time_in_millis" : 0,
           "current" : 0,
           "memory_size_in_bytes" : -1,
           "memory_size" : "-1b",
           "queries" : 0
         },
         "completion" : {
           "size_in_bytes" : 0
         },
         "segments" : {
           "count" : 0,
           "memory_in_bytes" : 0,
           "terms_memory_in_bytes" : 0,
           "stored_fields_memory_in_bytes" : 0,
           "term_vectors_memory_in_bytes" : 0,
           "norms_memory_in_bytes" : 0,
           "doc_values_memory_in_bytes" : 0,
           "index_writer_memory_in_bytes" : 0,
           "index_writer_max_memory_in_bytes" : 0,
           "version_map_memory_in_bytes" : 0,
           "fixed_bit_set_memory_in_bytes" : 0
         },
         "translog" : {
           "operations" : 0,
           "size_in_bytes" : 0
         },
         "suggest" : {
           "total" : 0,
           "time_in_millis" : 0,
           "current" : 0
         },
         "request_cache" : {
           "memory_size_in_bytes" : 0,
           "evictions" : 0,
           "hit_count" : 0,
           "miss_count" : 0
         },
         "recovery" : {
           "current_as_source" : 0,
           "current_as_target" : 0,
           "throttle_time_in_millis" : 0
         }
       },
       "os" : {
         "timestamp" : 1499371019779,
         "cpu_percent" : 9,
         "load_average" : 0.35,
         "mem" : {
           "total_in_bytes" : 25281347584,
           "free_in_bytes" : 14879522816,
           "used_in_bytes" : 10401824768,
           "free_percent" : 59,
           "used_percent" : 41
         },
         "swap" : {
           "total_in_bytes" : 536866816,
           "free_in_bytes" : 469614592,
           "used_in_bytes" : 67252224
         }
       },
       "process" : {
         "timestamp" : 1499371019779,
         "open_file_descriptors" : 287,
         "max_file_descriptors" : 1048576,
         "cpu" : {
           "percent" : 0,
           "total_in_millis" : 112210
         },
         "mem" : {
           "total_virtual_in_bytes" : 7164125184
         }
       },
       "jvm" : {
         "timestamp" : 1499371019779,
         "uptime_in_millis" : 3244443,
         "mem" : {
           "heap_used_in_bytes" : 100580080,
           "heap_used_percent" : 9,
           "heap_committed_in_bytes" : 1037959168,
           "heap_max_in_bytes" : 1037959168,
           "non_heap_used_in_bytes" : 98172920,
           "non_heap_committed_in_bytes" : 100057088,
           "pools" : {
             "young" : {
               "used_in_bytes" : 24878512,
               "max_in_bytes" : 286326784,
               "peak_used_in_bytes" : 286326784,
               "peak_max_in_bytes" : 286326784
             },
             "survivor" : {
               "used_in_bytes" : 2815560,
               "max_in_bytes" : 35782656,
               "peak_used_in_bytes" : 35782656,
               "peak_max_in_bytes" : 35782656
             },
             "old" : {
               "used_in_bytes" : 72886008,
               "max_in_bytes" : 715849728,
               "peak_used_in_bytes" : 72886008,
               "peak_max_in_bytes" : 715849728
             }
           }
         },
         "threads" : {
           "count" : 114,
           "peak_count" : 119
         },
         "gc" : {
           "collectors" : {
             "young" : {
               "collection_count" : 11,
               "collection_time_in_millis" : 749
             },
             "old" : {
               "collection_count" : 2,
               "collection_time_in_millis" : 198
             }
           }
         },
         "buffer_pools" : {
           "direct" : {
             "count" : 61,
             "used_in_bytes" : 15763457,
             "total_capacity_in_bytes" : 15763456
           },
           "mapped" : {
             "count" : 0,
             "used_in_bytes" : 0,
             "total_capacity_in_bytes" : 0
           }
         }
       },
       "thread_pool" : {
         "bulk" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "fetch_shard_started" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "fetch_shard_store" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "flush" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "force_merge" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "generic" : {
           "threads" : 1,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 5,
           "completed" : 3591
         },
         "get" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "index" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "listener" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "management" : {
           "threads" : 1,
           "queue" : 0,
           "active" : 1,
           "rejected" : 0,
           "largest" : 1,
           "completed" : 0
         },
         "percolate" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "refresh" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "search" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "snapshot" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "suggest" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "warmer" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         }
       },
       "fs" : {
         "timestamp" : 1499371019781,
         "total" : { },
         "data" : [ ]
       },
       "transport" : {
         "server_open" : 26,
         "rx_count" : 7145,
         "rx_size_in_bytes" : 2045345,
         "tx_count" : 7144,
         "tx_size_in_bytes" : 512929
       },
       "breakers" : {
         "request" : {
           "limit_size_in_bytes" : 415183667,
           "limit_size" : "395.9mb",
           "estimated_size_in_bytes" : 0,
           "estimated_size" : "0b",
           "overhead" : 1.0,
           "tripped" : 0
         },
         "fielddata" : {
           "limit_size_in_bytes" : 622775500,
           "limit_size" : "593.9mb",
           "estimated_size_in_bytes" : 0,
           "estimated_size" : "0b",
           "overhead" : 1.03,
           "tripped" : 0
         },
         "in_flight_requests" : {
           "limit_size_in_bytes" : 1037959168,
           "limit_size" : "989.8mb",
           "estimated_size_in_bytes" : 123,
           "estimated_size" : "123b",
           "overhead" : 1.0,
           "tripped" : 0
         },
         "parent" : {
           "limit_size_in_bytes" : 726571417,
           "limit_size" : "692.9mb",
           "estimated_size_in_bytes" : 123,
           "estimated_size" : "123b",
           "overhead" : 1.0,
           "tripped" : 0
         }
       },
       "script" : {
         "compilations" : 0,
         "cache_evictions" : 0
       }
     },
     "9kIDVbmgSTC1xjrTqALhzQ" : {
       "timestamp" : 1499371019769,
       "name" : "all-in-one-DockerDeployAllInOne-7node",
       "transport_address" : "172.17.0.3:9302",
       "host" : "172.17.0.3",
       "ip" : [ "172.17.0.3:9302", "NONE" ],
       "attributes" : {
         "data" : "false",
         "master" : "false"
       },
       "indices" : {
         "docs" : {
           "count" : 0,
           "deleted" : 0
         },
         "store" : {
           "size_in_bytes" : 0,
           "throttle_time_in_millis" : 0
         },
         "indexing" : {
           "index_total" : 0,
           "index_time_in_millis" : 0,
           "index_current" : 0,
           "index_failed" : 0,
           "delete_total" : 0,
           "delete_time_in_millis" : 0,
           "delete_current" : 0,
           "noop_update_total" : 0,
           "is_throttled" : false,
           "throttle_time_in_millis" : 0
         },
         "get" : {
           "total" : 0,
           "time_in_millis" : 0,
           "exists_total" : 0,
           "exists_time_in_millis" : 0,
           "missing_total" : 0,
           "missing_time_in_millis" : 0,
           "current" : 0
         },
         "search" : {
           "open_contexts" : 0,
           "query_total" : 0,
           "query_time_in_millis" : 0,
           "query_current" : 0,
           "fetch_total" : 0,
           "fetch_time_in_millis" : 0,
           "fetch_current" : 0,
           "scroll_total" : 0,
           "scroll_time_in_millis" : 0,
           "scroll_current" : 0
         },
         "merges" : {
           "current" : 0,
           "current_docs" : 0,
           "current_size_in_bytes" : 0,
           "total" : 0,
           "total_time_in_millis" : 0,
           "total_docs" : 0,
           "total_size_in_bytes" : 0,
           "total_stopped_time_in_millis" : 0,
           "total_throttled_time_in_millis" : 0,
           "total_auto_throttle_in_bytes" : 0
         },
         "refresh" : {
           "total" : 0,
           "total_time_in_millis" : 0
         },
         "flush" : {
           "total" : 0,
           "total_time_in_millis" : 0
         },
         "warmer" : {
           "current" : 0,
           "total" : 0,
           "total_time_in_millis" : 0
         },
         "query_cache" : {
           "memory_size_in_bytes" : 0,
           "total_count" : 0,
           "hit_count" : 0,
           "miss_count" : 0,
           "cache_size" : 0,
           "cache_count" : 0,
           "evictions" : 0
         },
         "fielddata" : {
           "memory_size_in_bytes" : 0,
           "evictions" : 0
         },
         "percolate" : {
           "total" : 0,
           "time_in_millis" : 0,
           "current" : 0,
           "memory_size_in_bytes" : -1,
           "memory_size" : "-1b",
           "queries" : 0
         },
         "completion" : {
           "size_in_bytes" : 0
         },
         "segments" : {
           "count" : 0,
           "memory_in_bytes" : 0,
           "terms_memory_in_bytes" : 0,
           "stored_fields_memory_in_bytes" : 0,
           "term_vectors_memory_in_bytes" : 0,
           "norms_memory_in_bytes" : 0,
           "doc_values_memory_in_bytes" : 0,
           "index_writer_memory_in_bytes" : 0,
           "index_writer_max_memory_in_bytes" : 0,
           "version_map_memory_in_bytes" : 0,
           "fixed_bit_set_memory_in_bytes" : 0
         },
         "translog" : {
           "operations" : 0,
           "size_in_bytes" : 0
         },
         "suggest" : {
           "total" : 0,
           "time_in_millis" : 0,
           "current" : 0
         },
         "request_cache" : {
           "memory_size_in_bytes" : 0,
           "evictions" : 0,
           "hit_count" : 0,
           "miss_count" : 0
         },
         "recovery" : {
           "current_as_source" : 0,
           "current_as_target" : 0,
           "throttle_time_in_millis" : 0
         }
       },
       "os" : {
         "timestamp" : 1499371019780,
         "cpu_percent" : 9,
         "load_average" : 0.35,
         "mem" : {
           "total_in_bytes" : 25281347584,
           "free_in_bytes" : 14879522816,
           "used_in_bytes" : 10401824768,
           "free_percent" : 59,
           "used_percent" : 41
         },
         "swap" : {
           "total_in_bytes" : 536866816,
           "free_in_bytes" : 469614592,
           "used_in_bytes" : 67252224
         }
       },
       "process" : {
         "timestamp" : 1499371019781,
         "open_file_descriptors" : 461,
         "max_file_descriptors" : 8192,
         "cpu" : {
           "percent" : 1,
           "total_in_millis" : 525180
         },
         "mem" : {
           "total_virtual_in_bytes" : 8468897792
         }
       },
       "jvm" : {
         "timestamp" : 1499371019782,
         "uptime_in_millis" : 3214518,
         "mem" : {
           "heap_used_in_bytes" : 199268424,
           "heap_used_percent" : 9,
           "heap_committed_in_bytes" : 1004011520,
           "heap_max_in_bytes" : 2077753344,
           "non_heap_used_in_bytes" : 173599616,
           "non_heap_committed_in_bytes" : 176484352,
           "pools" : {
             "young" : {
               "used_in_bytes" : 77384992,
               "max_in_bytes" : 558432256,
               "peak_used_in_bytes" : 558432256,
               "peak_max_in_bytes" : 558432256
             },
             "survivor" : {
               "used_in_bytes" : 16611488,
               "max_in_bytes" : 69730304,
               "peak_used_in_bytes" : 69730304,
               "peak_max_in_bytes" : 69730304
             },
             "old" : {
               "used_in_bytes" : 105271944,
               "max_in_bytes" : 1449590784,
               "peak_used_in_bytes" : 354313784,
               "peak_max_in_bytes" : 1449590784
             }
           }
         },
         "threads" : {
           "count" : 253,
           "peak_count" : 258
         },
         "gc" : {
           "collectors" : {
             "young" : {
               "collection_count" : 48,
               "collection_time_in_millis" : 2044
             },
             "old" : {
               "collection_count" : 5,
               "collection_time_in_millis" : 761
             }
           }
         },
         "buffer_pools" : {
           "direct" : {
             "count" : 164,
             "used_in_bytes" : 27716184,
             "total_capacity_in_bytes" : 27716183
           },
           "mapped" : {
             "count" : 0,
             "used_in_bytes" : 0,
             "total_capacity_in_bytes" : 0
           }
         }
       },
       "thread_pool" : {
         "bulk" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "fetch_shard_started" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "fetch_shard_store" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "flush" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "force_merge" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "generic" : {
           "threads" : 1,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 5,
           "completed" : 3549
         },
         "get" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "index" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "listener" : {
           "threads" : 4,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 4,
           "completed" : 2228
         },
         "management" : {
           "threads" : 1,
           "queue" : 0,
           "active" : 1,
           "rejected" : 0,
           "largest" : 1,
           "completed" : 0
         },
         "percolate" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "refresh" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "search" : {
           "threads" : 13,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 13,
           "completed" : 13719
         },
         "snapshot" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "suggest" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         },
         "warmer" : {
           "threads" : 0,
           "queue" : 0,
           "active" : 0,
           "rejected" : 0,
           "largest" : 0,
           "completed" : 0
         }
       },
       "fs" : {
         "timestamp" : 1499371019784,
         "total" : { },
         "data" : [ ]
       },
       "transport" : {
         "server_open" : 26,
         "rx_count" : 74689,
         "rx_size_in_bytes" : 43844130,
         "tx_count" : 74672,
         "tx_size_in_bytes" : 63989907
       },
       "breakers" : {
         "request" : {
           "limit_size_in_bytes" : 831101337,
           "limit_size" : "792.5mb",
           "estimated_size_in_bytes" : 0,
           "estimated_size" : "0b",
           "overhead" : 1.0,
           "tripped" : 0
         },
         "fielddata" : {
           "limit_size_in_bytes" : 1246652006,
           "limit_size" : "1.1gb",
           "estimated_size_in_bytes" : 0,
           "estimated_size" : "0b",
           "overhead" : 1.03,
           "tripped" : 0
         },
         "in_flight_requests" : {
           "limit_size_in_bytes" : 2077753344,
           "limit_size" : "1.9gb",
           "estimated_size_in_bytes" : 123,
           "estimated_size" : "123b",
           "overhead" : 1.0,
           "tripped" : 0
         },
         "parent" : {
           "limit_size_in_bytes" : 1454427340,
           "limit_size" : "1.3gb",
           "estimated_size_in_bytes" : 123,
           "estimated_size" : "123b",
           "overhead" : 1.0,
           "tripped" : 0
         }
       },
       "script" : {
         "compilations" : 0,
         "cache_evictions" : 0
       }
     }
   }
 }
 #+END_EXAMPLE
*** DONE ES get allocation: curl $es_ip:9200/_cat/allocation?v
    CLOSED: [2017-08-10 Thu 14:37]
   https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-allocation.html
 allocation provides a snapshot of how many shards are allocated to each data node and how much disk space they are using.
*** DONE ES disk watermark: 85%, 90%
    CLOSED: [2017-08-10 Thu 15:49]
 https://www.elastic.co/guide/en/elasticsearch/reference/current/disk-allocator.html
 - cluster.routing.allocation.disk.watermark.low: defaults to 85%
   meaning ES will not allocate new shards to nodes once they have more than 85% disk used

 - cluster.routing.allocation.disk.watermark.high: defaults to 90%
   meaning ES will attempt to relocate shards to another node if the node disk usage rises above 90%.
*** DONE [#A] Elasticsearch Total Shards Per Node
    CLOSED: [2017-08-10 Thu 15:49]
 https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-total-shards.html
 https://github.com/elastic/elasticsearch/issues/20705
 https://discuss.elastic.co/t/total-shards-per-node/39887

 - index.routing.allocation.total_shards_per_node
 - cluster.routing.allocation.total_shards_per_node

 curl http://$es_ip:9200/_cluster/settings?v
*** [#A] Lessons Learned For Cost Saving Of Elasticsearch Cluster
 https://github.com/TOTVS/mdmdevops/wiki/Lessons-Learned-For-Cost-Saving-Of-Elasticsearch-Cluster
 #+BEGIN_EXAMPLE
 What For?
 Elasticsearch cluster is one of the key factors in our cost analysis.

 Use less VMs and Volumes for ES, but still:

 Serve the same amount the data and query.
 Performance should be acceptable.
 Typical Questions:

 Can I use less VMs?
 Why shard re-allocation doesn't happen, when nodes are so unbalanced?
 Key factors
 Use More Cache

 Fielddata: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-fielddata.html

 indices.fielddata.cache.size
 Thread Pool: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-threadpool.html

 threadpool.index.queue_size
 threadpool.bulk.queue_size
 Self Protection

 Circuit breaker: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/circuit-breaker.html

 indices.breaker.total.limit

 indices.breaker.fielddata.limit

 indices.breaker.request.limit

 index.query.bool.max_clause_count

 Shard Size

 "Too big shards" VS "Too many shards": Ideal shard should be no bigger than 20GB.
 Shard Allocation

 ES rebalancing for data paths: https://stackoverflow.com/questions/36737023/elasticsearchhow-to-make-a-balance-of-the-usage-of-disk?noredirect=1#comment61059094_36737023
 - ES will not try to balance the files between paths.

 - When it decides to allocate a new shard to a node with multiple
   paths it will put it on the path with the most free space. But it
   will not move one shard from one path and put it on another
   path. The rebalancing happens between nodes and not between data
   paths.
 Shards re-allocation in different nodes are based on shard count, instead of disk size.
 If you have very few huge shards and many small shards, the ES cluster may not be balanced.

 Impact?

 Some nodes may hold many giant shards with high disk utilization. Meanwhile other nodes are holding many small shards but low disk utilization.
 The nodes holding many giant shards will be the performance bottlenecks. These nodes will have high CPU, while other nodes have very low CPU. And your application is slow.
 Total Shards Per Node

 https://www.elastic.co/guide/en/elasticsearch/reference/2.3/allocation-total-shards.html

 index.routing.allocation.total_shards_per_node
 cluster.routing.allocation.total_shards_per_node
 #+END_EXAMPLE
*** TODO [#A] Werid ES shard re-allocation logic
 #+BEGIN_EXAMPLE
 Denny Zhang
 [6:54 PM] 
 Found an interesting ES shard re-allocation.

 ```root@bematech-do-es-18:/usr/share/elasticsearch/mdm/nodes/0/indices# curl $es_ip:9200/_cat/shards?v | grep -v STARTED
 index                                               shard prirep state          docs   store ip              node
 master-index-abae8b30ac9b11e692000401f8d88101-new3  27    p      RELOCATING 27176363  34.8gb 165.227.25.58   bematech-do-es-16.localdomain -> 165.227.25.149 2ScQBOZqQVCOKOdztMtHjA bematech-do-es-18.localdomain
 master-index-abae8b30ac9b11e692000401f8d88101-new3  30    r      RELOCATING 27229987  35.8gb 165.227.25.149  bematech-do-es-18.localdomain -> 165.227.25.58 f4DCO3_VRzGZpOQyp19O1g bematech-do-es-16.localdomain
 ```

 ```root@bematech-do-es-18:/usr/share/elasticsearch/mdm/nodes/0/indices# du  -h -d 1 /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3
 36G     /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3/30
 35G     /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3/23
 35G     /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3/28
 36G     /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3/25
 36G     /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3/9
 36G     /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3/12
 35G     /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3/11
 56K     /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3/_state
 3.9G    /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3/27
 251G    /usr/share/elasticsearch/mdm/nodes/0/indices/master-index-abae8b30ac9b11e692000401f8d88101-new3
 ```


 [6:54] 
  ```root@bematech-do-es-18:/usr/share/elasticsearch/mdm/nodes/0/indices# df -h
 Filesystem      Size  Used Avail Use% Mounted on
 udev             16G   12K   16G   1% /dev
 tmpfs           3.2G  380K  3.2G   1% /run
 /dev/vda1       315G  254G   50G  84% /
 none            4.0K     0  4.0K   0% /sys/fs/cgroup
 none            5.0M     0  5.0M   0% /run/lock
 none             16G     0   16G   0% /run/shm
 none            100M     0  100M   0% /run/user
 /dev/sda        689G  489G  166G  75% /data/elasticsearch2
 ```

 For bematech-do-es-18, we see replica 27 move to this node, then replica 30 move out of the node.


 [6:55] 
 Don't quite understand this logic. *What for?*

 I will check more about this strange behavior.
 #+END_EXAMPLE
*** ES won't balance the data between the data directories in the same way as shards are balanced between nodes in a cluster. 
 https://discuss.elastic.co/t/two-different-partitions-in-the-same-elaticsearch-server/31280/7

 The newly added partition will only be considered for newly created files
*** DONE ES rebalancing for data paths
    CLOSED: [2017-08-09 Wed 17:22]

 https://stackoverflow.com/questions/36737023/elasticsearchhow-to-make-a-balance-of-the-usage-of-disk?noredirect=1#comment61059094_36737023

 - ES will not try to balance the files between paths.

 - When it decides to allocate a new shard to a node with multiple
   paths it will put it on the path with the most free space. But it
   will not move one shard from one path and put it on another
   path. The rebalancing happens between nodes and not between data
   paths.
*** #  --8<-------------------------- separator ------------------------>8--
*** DONE enable elasticsearch gc: ES_USE_GC_LOGGING
    CLOSED: [2017-08-20 Sun 09:18]
 http://tech.taskrabbit.com/blog/2014/07/18/elasticsearch-in-production/

 [5:53] 
 https://discuss.elastic.co/t/how-to-enable-garbage-collector-logging-in-an-elastic-search-nodes/22487


 [5:53] 
 ES_USE_GC_LOGGING
*** DONE [#A] When backup elasticsearch, will write requests be freezed: The snapshotting process is executed in a non-blocking fashion
    CLOSED: [2017-08-21 Mon 13:17]
 https://dzone.com/articles/introduction-elasticsearch-0

 The snapshotting process is executed in a non-blocking fashion by
 Elasticsearch. This means all operations can continue to be executed
 against the index during snapshotting.
*** TODO elasticsearch delete matched documents
 index=logstash-2016.10.04

 curl -XDELETE "localhost:9200/$index/_query" \
 -d '{
     "query" : {
         "query_string":
         {"query": "(item_name:HTTPCode)" }
     }
 }'
*** DONE elasticdump
    CLOSED: [2017-08-25 Fri 17:56]
 elasticdump \
   --input=http://138.197.215.132:9200/master-abae8b30ac9b11e692000401f8d88101 \
   --output=http://165.227.19.83:9200/master-index-abae8b30ac9b11e692000401f8d88101 \
   --type=data
*** DONE Speed up elasticsearch service start: make sure more than 2 shards are in initializing status
    CLOSED: [2017-08-22 Tue 11:14]
 #+BEGIN_EXAMPLE
 Denny Zhang
 [4:17 PM] 
 Currently restarting ES instances usually takes 4+ hours.

 Found one improvement to speed up this process

 We can check cluster status by "curl $es_ip:9200/_cluster/health?pretty",

 After service start, we will see always 2 shards are in initializing status (See:  "initializing_shards" : 2,)

 ```  "active_primary_shards" : 591,
   "active_shards" : 1708,
   "relocating_shards" : 1,
   "initializing_shards" : 2,
   "unassigned_shards" : 27,
   "delayed_unassigned_shards" : 0,
   "active_shards_percent_as_number" : 98.27387802071347
 ```

 If we can have more shards run initializing, the es restart should be faster.

 Changing cluster.routing.allocation.node_concurrent_recoveries to 3, we will see 3 shards in initializing status.
 https://www.elastic.co/guide/en/elasticsearch/reference/2.3/shards-allocation.html

 I will configure it to 3 temporarily. After the restart is done, I will change it back to default, which is 2.

 Kung Wang
 [4:19 PM] 
 nice, there are pros and cons on this parameter. I found this one as well yesterday, but for SSD, increase this parameter should be ok. we need to monitor


 Denny Zhang
 [4:19 PM] 
 Yes, from 2 to 3, should be safe for us.


 Kung Wang [4:21 PM] 
 ```Pros: 
 - recovery may be faster 

 Cons 
 - recovery takes many network resources 
 - queries and indexing may not respond in time 
 - higher iowaits 
 - network bandwidth must be available 

 The best method to achieve quick recovery is to select a wise shards per 
 node ratio and a sane shard size. 

 Here is my approach: on my 32-core machines, I plan to never run more than 
 32 shards, and no shard shall grow beyond 5-10g, so transporting on a 
 10GBit/s takes reasonable time. 
 ```
 Denny Zhang
 [4:22 PM] 
 It's initializing. Mostly it should be loading locally.

 So no too many network traffic.

 Surely disk will be busier. I agree. Considering to current ES load, it should be fine to push harder.


 [4:23] 
 I'm comfortable with the parameter configured as 3 temporarily. But 4, I'm not sure.
 #+END_EXAMPLE
**** scripts
 #+BEGIN_EXAMPLE
 curl -XPUT 'http://escluster:9200/_cluster/settings' -d '{
  "transient" : {
  "cluster.routing.allocation.cluster_concurrent_rebalance": 20,
  "indices.recovery.concurrent_streams": 20,
  "cluster.routing.allocation.node_initial_primaries_recoveries": 20,
  "cluster.routing.allocation.node_concurrent_recoveries": 20,
  "indices.recovery.max_bytes_per_sec": "2048mb",
  "cluster.routing.allocation.disk.threshold_enabled" : true,
  "cluster.routing.allocation.disk.watermark.low" : "90%",
  "cluster.routing.allocation.disk.watermark.high" : "98%",
  "cluster.routing.allocation.enable": "primary"
  }
 }'
 #+END_EXAMPLE
**** useful link
 https://t37.net/an-elasticsearch-cheat-sheet.html
 https://t37.net/how-to-fix-your-elasticsearch-cluster-stuck-in-initializing-shards-mode.html  
*** DONE File system storage types: niofs, default_fs
    CLOSED: [2017-08-31 Thu 15:04]
 https://www.elastic.co/guide/en/elasticsearch/reference/2.4/index-modules-store.html
*** DONE elasticsearch exclude a node
    CLOSED: [2017-08-31 Thu 22:38]
 curl -XPUT $es_ip:9200/_cluster/settings -d '{
   "transient" :{
       "cluster.routing.allocation.exclude._ip" : "138.68.50.117"
    }
 }'
*** DONE elasticsearch delete closed indice
    CLOSED: [2017-08-31 Thu 18:20]
 for index in $(curl $es_ip:9200/_cat/indices?v | grep "close" | awk -F" " '{print $2}'); do
      echo "curl -XDELETE http://$es_ip:9200/$index"
      #curl -XDELETE "http://$es_ip:9200/$index"
 done

 curl $es_ip:9200/_cat/indices?v
*** DONE Get ES index create time: creation_date
    CLOSED: [2017-09-07 Thu 22:13]
*** DONE [#A] convert ES index creation_date to human readable format
    CLOSED: [2017-09-07 Thu 22:13]
 - The creation date is given with millisecond precision. Take away the last 3 digits and you converter gives Fri, 06 Mar 2015 08:44:57 GMT for 1425631497.

 date -d : epoch timestamp to human readable format: doesn't seem to be correct
 http://elasticsearch-users.115913.n3.nabble.com/creation-date-in-index-setteing-td4073837.html
 https://www.epochconverter.com

 staging-index-799e458055c611e6bb000401f8d88101: creation_date 1469822162361 (Sun Oct 23 13:59:21)
 master-index-860245c0841e11e6a8260401f8d88101: creation_date 1474917731982 (Tue Apr 14 01:59:42)
 staging-index-860245c0841e11e6a8260401f8d88101: creation_date 1474917732200 (Tue Apr 14 02:03:20)
 master-index-799e458055c611e6bb000401f8d88101: creation_date 1469822162085 (Sun Oct 23 13:54:45)
 master-index-3fa847206b9911e6b61d0401f8d88101: creation_date 1472221662989 (Wed Nov  6 13:56:29)
 staging-index-3366bf206f2a11e694f80401f8d88c01: creation_date 1472613773742 (Fri Apr 10 21:35:42)
 master-index-3366bf206f2a11e694f80401f8d88c01: creation_date 1472613773062 (Fri Apr 10 21:24:22)
 staging-index-3fa847206b9911e6b61d0401f8d88101: creation_date 1472221663267 (Wed Nov  6 14:01:07)
*** DONE detect elasticsearch full gc
    CLOSED: [2017-09-13 Wed 09:20]
 https://stackoverflow.com/questions/895444/java-garbage-collection-log-messages
*** #  --8<-------------------------- separator ------------------------>8--
*** DONE Kung scripts about ES rebalancing
    CLOSED: [2017-09-14 Thu 15:28]
 #+BEGIN_EXAMPLE
 #!/bin/bash

 function how_many_cpus {
 getconf _NPROCESSORS_ONLN
 }

 function hostport {
 host=$1
 port=$2
 portex=""
 if [ "$port" != "" ]; then
   portex=":$port"
 fi
 echo $host$portex
 }

 # Transient
 #   These changes are in effect until the cluster restarts. 
 #   Once a full cluster restart takes place, these settings are erased.
 # Persistent
 #   These changes are permanently in place unless explicitly changed. 
 #   They will survive full cluster restarts and override the static configuration files.


 function remove_cluster_routing_exclude {
 host=$(hostport $1 $2)
 curl -XPUT http://$host/_cluster/settings -d '{"transient":{"cluster.routing.allocation.exclude._ip":""}}' 2>/dev/null
 curl -XGET http://$host/_cluster/settings 2>/dev/null | jq '.'
 }

 # ----------- shard allocation (dynamic settings, no need to restart to take effect) -------------

 function update_shard_allocation_settings {
 # all - (default) Allows shard allocation for all kinds of shards.
 # primaries - Allows shard allocation only for primary shards.
 # new_primaries - Allows shard allocation only for primary shards for new indices.
 # none - No shard allocations of any kind are allowed for any indices.
 host=$(hostport $1 $2)
 value=$3
 curl -XPUT http://$host/_cluster/settings -d '{"transient":{"cluster.routing.allocation.enable":"'$value'"}}' 2>/dev/null
 }

 function turn_on_shard_allocation {
 update_shard_allocation_settings $1 $2 all
 }

 function turn_off_shard_allocation {
 update_shard_allocation_settings $1 $2 none
 }

 function update_number_of_concurrent_recovery_threads {
 # How many concurrent shard recoveries are allowed to happen on a node. Defaults to 2.
 host=$(hostport $1 $2)
 value=$3
 curl -XPUT http://$host/_cluster/settings -d '{"transient":{"cluster.routing.allocation.node_concurrent_recoveries":"'$value'"}}' 2>/dev/null
 }

 function update_number_of_initial_primary_recovery_threads {
 # While the recovery of replicas happens over the network, 
 # the recovery of an unassigned primary after node restart uses data from the local disk. 
 # These should be fast so more initial primary recoveries can happen in parallel on the same node. 
 # Defaults to 4.
 host=$(hostport $1 $2)
 value=$3
 curl -XPUT http://$host/_cluster/settings -d '{"transient":{"cluster.routing.allocation.node_initial_primaries_recoveries":"'$value'"}}' 2>/dev/null
 }

 function enable_shard_allocation {
 update_shard_allocation_settings $1 $2 all
 }

 function disable_shard_allocation {
 update_shard_allocation_settings $1 $2 none
 }

 # ----------- recovery (dynamic settings) -------------

 function put_index_higher_priorty_to_recover {
 # Unallocated shards are recovered in order of priority, whenever possible. Indices are sorted into priority order as follows:
 # the optional index.priority setting (higher before lower)
 # the index creation date (higher before lower)
 # the index name (higher before lower)
 host=$(hostport $1 $2)
 index=$3
 curl -XPUT http://$host/$index -d '{"settings":{"index.priority":9999}}' 2>/dev/null
 }

 function update_index_recovery_threshold {
 host=$(hostport $1 $2)
 index=$3
 curl -XPUT http://$host/_cluster/settings -d '{"transient":{"indices.recovery.max_bytes_per_sec":"'$value'"}}' 2>/dev/null
 }

 function increase_recovery_speed {
 # this function will use number_of_cpus * 2 as new number for node_concurrent_recoveries
 update_number_of_concurrent_recovery_threads $1 $2 "$((`how_many_cpus`*2))"
 update_index_recovery_threshold $1 $2 "200mb"
 }


 # ----------- shard rebalance (dynamic settings, no need to restart to take effect) -------------

 function update_shard_type_rebalance_settings {
 # all - (default) Allows shard balancing for all kinds of shards.
 # primaries - Allows shard balancing only for primary shards.
 # replicas - Allows shard balancing only for replica shards.
 # none - No shard balancing of any kind are allowed for any indices.
 host=$(hostport $1 $2)
 value=$3
 curl -XPUT http://$host/_cluster/settings -d '{"transient":{"cluster.routing.rebalance.enable":"'$value'"}}' 2>/dev/null
 }

 function update_shard_allowance_rebalance_settings {
 # always - Always allow rebalancing.
 # indices_primaries_active - Only when all primaries in the cluster are allocated.
 # indices_all_active - (default) Only when all shards (primaries and replicas) in the cluster are allocated.
 host=$(hostport $1 $2)
 value=$3
 curl -XPUT http://$host/_cluster/settings -d '{"transient":{"cluster.routing.allocation.allow_rebalance":"'$value'"}}' 2>/dev/null
 }

 function update_shard_rebalance_concurrent_threads {
 # Allow to control how many concurrent shard rebalances are allowed cluster wide. Defaults to 2
 # Note that this setting only controls the number of concurrent shard relocations due to imbalances in the cluster. 
 # This setting does not limit shard relocations due to allocation filtering or forced awareness.
 host=$(hostport $1 $2)
 value=$3
 curl -XPUT http://$host/_cluster/settings -d '{"transient":{"cluster.routing.allocation.cluster_concurrent_rebalance":"'$value'"}}' 2>/dev/null
 }

 function increase_rebalance_speed {
 # this function will use number_of_cpus * 2 as new number for cluster_concurrent_rebalance
 # we need to also increase node_concurrent_recoveries, as rebalance also use recovery threads
 value="$((`how_many_cpus`*2))"
 update_number_of_concurrent_recovery_threads $1 $2 $value
 update_shard_rebalance_concurrent_threads $1 $2 $value
 }

 # https://www.elastic.co/guide/en/elasticsearch/reference/5.5/shards-allocation.html
 # please see Shard Balancing Heuristics if it's needed


 # ----------- recovery status -------------

 function cluster_wide_recovery_status {
 # https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-recovery.html
 host=$(hostport $1 $2)
 curl -XGET http://$host/_recovery?detailed=true&active_only=true 2>/dev/null
 }

 function index_recovery_status {
 # https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-recovery.html
 host=$(hostport $1 $2)
 index=$3
 curl -XGET http://$host/$index/_recovery?detailed=true&active_only=true 2>/dev/null
 }

 #+END_EXAMPLE
*** web page: Cluster Level Shard Allocation | Elasticsearch Reference [2.3] | Elastic          
 https://www.elastic.co/guide/en/elasticsearch/reference/2.3/shards-allocation.html
**** webcontent                                                    :noexport:
 #+begin_example
 Location: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/shards-allocation.html                                   
 Questions? Feedback? powered by Olark live chat software

 elastic-logo-mobile

   * 
   * 
   * EN
       + English
       + Français
       + Deutsch
       + 日本語
       + 한국어
       + 简体中文

 elastic-logo

   * Products
   * Cloud
   * Services
   * Customers
   * Learn

   * downloads
   * 
   * contact
   * 
   * 
   * EN
       + English
       + Français
       + Deutsch
       + 日本語
       + 한국어
       + 简体中文

   * [                    ]

 Docs
 Docs
 You are looking at documentation for an older release. Not what you want? See the current release
 documentation.
 Elasticsearch Reference [2.3] » Modules » Cluster » Cluster Level Shard Allocation
 «  Cluster     Disk-based Shard Allocation  »

 Cluster Level Shard Allocationedit

 Shard allocation is the process of allocating shards to nodes. This can happen during initial
 recovery, replica allocation, rebalancing, or when nodes are added or removed.

 Shard Allocation Settingsedit

 The following dynamic settings may be used to control shard allocation and recovery:

 cluster.routing.allocation.enable
   
     Enable or disable allocation for specific kinds of shards:
   
       * all - (default) Allows shard allocation for all kinds of shards.
       * primaries - Allows shard allocation only for primary shards.
       * new_primaries - Allows shard allocation only for primary shards for new indices.
       * none - No shard allocations of any kind are allowed for any indices.
   
     This setting does not affect the recovery of local primary shards when restarting a node. A
     restarted node that has a copy of an unassigned primary shard will recover that primary
     immediately, assuming that the index.recovery.initial_shards setting is satisfied.
   
 cluster.routing.allocation.node_concurrent_recoveries
     How many concurrent shard recoveries are allowed to happen on a node. Defaults to 2.
 cluster.routing.allocation.node_initial_primaries_recoveries
     While the recovery of replicas happens over the network, the recovery of an unassigned primary
     after node restart uses data from the local disk. These should be fast so more initial primary
     recoveries can happen in parallel on the same node. Defaults to 4.
 cluster.routing.allocation.same_shard.host
     Allows to perform a check to prevent allocation of multiple instances of the same shard on a
     single host, based on host name and host address. Defaults to false, meaning that no check is
     performed by default. This setting only applies if multiple nodes are started on the same
     machine.
 indices.recovery.concurrent_streams
     The number of network streams to open per node to recover a shard from a peer shard. Defaults
     to 3.
 indices.recovery.concurrent_small_file_streams
     The number of streams to open per node for small files (under 5mb) to recover a shard from a
     peer shard. Defaults to 2.

 Shard Rebalancing Settingsedit

 The following dynamic settings may be used to control the rebalancing of shards across the cluster:

 cluster.routing.rebalance.enable
   
     Enable or disable rebalancing for specific kinds of shards:
   
       * all - (default) Allows shard balancing for all kinds of shards.
       * primaries - Allows shard balancing only for primary shards.
       * replicas - Allows shard balancing only for replica shards.
       * none - No shard balancing of any kind are allowed for any indices.
 cluster.routing.allocation.allow_rebalance
   
     Specify when shard rebalancing is allowed:
   
       * always - Always allow rebalancing.
       * indices_primaries_active - Only when all primaries in the cluster are allocated.
       * indices_all_active - (default) Only when all shards (primaries and replicas) in the cluster
         are allocated.
 cluster.routing.allocation.cluster_concurrent_rebalance
     Allow to control how many concurrent shard rebalances are allowed cluster wide. Defaults to 2.

 Shard Balancing Heuristicsedit

 The following settings are used together to determine where to place each shard. The cluster is
 balanced when no allowed action can bring the weights of each node closer together by more then the
 balance.threshold.

 cluster.routing.allocation.balance.shard
     Defines the weight factor for shards allocated on a node (float). Defaults to 0.45f. Raising
     this raises the tendency to equalize the number of shards across all nodes in the cluster.
 cluster.routing.allocation.balance.index
     Defines a factor to the number of shards per index allocated on a specific node (float).
     Defaults to 0.55f. Raising this raises the tendency to equalize the number of shards per index
     across all nodes in the cluster.
 cluster.routing.allocation.balance.threshold
     Minimal optimization value of operations that should be performed (non negative float).
     Defaults to 1.0f. Raising this will cause the cluster to be less aggressive about optimizing
     the shard balance.

 Note

 Regardless of the result of the balancing algorithm, rebalancing might not be allowed due to forced
 awareness or allocation filtering.

 «  Cluster     Disk-based Shard Allocation  »

 Getting Started Videos

   * Elasticsearch Demo
   * Kibana 101
   * Logstash Primer

 Be in the know with the latest and greatest from Elastic.

 Thanks for subscribing! We'll keep you updated with new releases.

 Products >

   * Elasticsearch
   * Kibana
   * Beats
   * Logstash
   * X-Pack
   * Elastic Cloud
   * Security (formerly Shield)
   * Alerting (via Watcher)
   * Monitoring (formerly Marvel)
   * Graph
   * Reporting
   * Machine Learning
   * ES-Hadoop

 Resources

   * Blog
   * Cloud Status
   * Community
   * Customers & Use Cases
   * Documentation
   * Elastic{ON} Events
   * Forums
   * Meetups
   * Subscriptions
   * Support Portal
   * Videos & Webinars
   * Training

 About >

   * Careers/Jobs
   * Contact
   * Leadership
   * Partners
   * Press
   * Elastic Store

 Language

   * English
   * Français
   * Deutsch
   * 日本語
   * 한국어
   * 简体中文

 FOLLOW US

   *  
   *  
   *  
   *  
   *  

   * Trademarks
   * Terms of Use
   * Privacy
   * Cookie Policy
   * Brand

 [logo-elast]

 © 2017. All Rights Reserved - Elasticsearch

 Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S. and in other countries

 Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of
 the Apache Software Foundation in the United States and/or other countries.

 #+end_example
*** DONE elasticsearch node leave timeout
    CLOSED: [2017-10-10 Tue 16:35]
 es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

 curl -XPUT "http://$es_ip:9200/_all/_settings" -d '
 {
   "settings": {
     "index.unassigned.node_left.delayed_timeout": "5m"
   }
 }'

 #+BEGIN_EXAMPLE
 Denny Zhang [11:14 AM] 
 Checked es-01. Only noticed this:
 1. CPU load was around 13. Now it's 2.77
 2. mdm.log of elasticsearch is as big as 6.1GB

 Root cause not clear. Here is our previous experience.
 If one ES node is slow to reply ES mgmt periodical check, ES cluster will consider the node is disconnected.


 Kung Wang
 [11:26 AM] 
 Here are settings we can tune in ES:
 discovery.zen.fd.ping_timeout = 30s
 discovery.zen.fd.ping_retries = 3
 discovery.zen.fd.ping_interval = 1s


 Denny Zhang [11:27 AM] 
 We double all these values?


 Kung Wang
 [11:29 AM] 
 if we delay the node leave timeout, then cluster won't go into yellow state that quick:
 ```PUT _all/_settings
 {
   "settings": {
     "index.unassigned.node_left.delayed_timeout": "5m"
   }
 }
 ```
 default is 1m


 [11:30] 
 we will need to test these values locally with smaller cluster and observe the behavior. then we decide


 [11:31] 
 @denny.zhang, can you add this ticket for yourself to perform this testings? then we can learn from it


 Denny Zhang [11:31 AM] 
 Sure


 new messages
 Kung Wang
 [11:32 AM] 
 bottom line is, if we can prevent cluster goes into yellow or red state, then we can maintain higher cluster availability
 #+END_EXAMPLE
*** DONE create elasticsearch index manually
    CLOSED: [2017-10-12 Thu 11:45]
 mkdir /tmp/master-index-64c1cfe0936611e7a3290e4789ade3a3

 cd  /tmp/master-index-64c1cfe0936611e7a3290e4789ade3a3

 es_port=9200
 old_index_name="master-index-64c1cfe0936611e7a3290e4789ade3a3"
 new_index_name="master-index-64c1cfe0936611e7a3290e4789ade3a3-new2"
 create_json_file="combined.json"
 create_timeout="30m"

 curl "http://${es_ip}:${es_port}/${old_index_name}/_settings" > settings.json
 cat mapping.json | jq --sort-keys '.' > mapping_sorted.json
 cat "mapping_sorted.json" "settings.json" | jq --slurp '.[0] * .[1]' > combined.json

 time curl -XPOST "http://${es_ip}:${es_port}/${new_index_name}?timeout=${create_timeout}&wait_for_active_shards=all" -d @"${create_json_file}" 
*** DONE When some app nodes stucks, es query node list API will stuck
    CLOSED: [2017-10-31 Tue 12:10]
 #+BEGIN_EXAMPLE
 create a jenkins job to move json files?


 [11:40] 
 Not sure why.

 But ES /nodes?v API doesn't return.

 Other APIs look good. Tenants GUI login is good.


 Bruno Volpato
 [11:42 AM] 
 maybe just one node is stuck?


 Denny Zhang [11:44 AM] 
 Tried es-1 and es-2


 Bruno Volpato
 [11:44 AM] 
 but I mean, it gets the load from all of them


 Denny Zhang [11:45 AM] 
 Checking the CPU load of all ES nodes, very low for all nodes.



 Nagios APP [11:49 AM] 
 bematech-do-app-03/check_mdm_threadcount is CRITICAL:
 CRITICAL: thread count of pid(16598) is 8873. Its more than 2560
 1 reply Today at 11:53 AM View thread


 Bruno Volpato
 [11:50 AM] 
 maybe it's coming from the appnodes.


 [11:50] 
 they are listed in the nodes


 [11:50] 
 let me stop them and see if it fixes


 Denny Zhang [11:50 AM] 
 Yes, I'm also thinking the same thing


 Bruno Volpato
 [11:50 AM] 
 let me just take a threaddump first


 Denny Zhang [11:51 AM] 
 Sure thing


 Bruno Volpato
 [11:51 AM] 
 yes, even jstack is stack on app-3

 [11:51] 
 root@bematech-do-app-03:~# jstack -F 16598
 Attaching to process ID 16598, please wait...
 Error attaching to process: sun.jvm.hotspot.debugger.DebuggerException: Can't attach to the process
 sun.jvm.hotspot.debugger.DebuggerException: sun.jvm.hotspot.debugger.DebuggerException: Can't attach to the process

 [11:54] 
 yes, it returns after killing app3 and app5


 Denny Zhang [11:54 AM] 
 Confirmed
 #+END_EXAMPLE
*** DONE [#A] ES check reindexing task
    CLOSED: [2018-01-19 Fri 14:02]
 https://www.elastic.co/guide/en/elasticsearch/reference/2.4/docs-reindex.html

 curl -XGET "http://$es_ip:9200/_tasks?detailed=true&actions=*reindex"

 While Reindex is running you can fetch their status using the Task API:

 GET /_tasks/?pretty&detailed=true&actions=*reindex

 Works with the Cancel Task APIedit
 Any Reindex can be canceled using the Task Cancel API:

 POST /_tasks/{task_id}/_cancel
 #+BEGIN_EXAMPLE
 Bruno Volpato [10:45 AM] 
 starting from `/_nodes/hot_threads`


 [10:47] 
 lots of merges going on with high loads, not sure why


 [10:47] 
 usually merges don't take 100%+ CPU


 Denny Zhang (DevOps) [10:47 AM] 
 We had re-indexing tasks issued, but the curl has been killed this morning.


 Bruno Volpato [10:47 AM] 
 123.0% (614.8ms out of 500ms) cpu usage by thread 'elasticsearch[bematech-do-es-9.localdomain][[master-index-0d60cee0843911e7b589dec7170128a9-new3][1]: Lucene Merge Thread #4154]'
    89.5% (447.3ms out of 500ms) cpu usage by thread 'elasticsearch[bematech-do-es-32.localdomain][[staging-index-0d60cee0843911e7b589dec7170128a9-new3][1]: Lucene Merge Thread #3772]'
    125.9% (629.4ms out of 500ms) cpu usage by thread 'elasticsearch[bematech-do-es-17.localdomain][[staging-index-0d60cee0843911e7b589dec7170128a9-new3][4]: Lucene Merge Thread #236]'


 [10:48] 
 I think it's still going on
 #+END_EXAMPLE
*** DONE Change ES configuration
    CLOSED: [2018-02-01 Thu 20:23]
 https://www.elastic.co/guide/en/elasticsearch/guide/2.x/logging.html

 es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')


 curl $es_ip:9200/_cat/indices?v | grep open | grep -v new3

 curl -XPUT $es_ip:9200/_cluster/settings -d '
 {
     "transient" : {
         "index.search.slowlog.threshold.query.warn" : "5s", 
         "index.indexing.slowlog.threshold.index.warn" : "6s" 
     }
 }'

 curl $es_ip:9200/_cluster/settings

 #+BEGIN_EXAMPLE
 Bruno Volpato [12:01 PM]
 can we reduce the %?
 since the RAM is pretty busy on the ES servers
 ```indices.breaker.fielddata.limit: 20%
 indices.breaker.request.limit: 10%
 indices.breaker.total.limit: 40%
 indices.fielddata.cache.size: 15%```
 let's bring up a node with this settings:

 ```indices.breaker.fielddata.limit: 20%
 indices.breaker.request.limit: 10%
 indices.breaker.total.limit: 35%
 indices.fielddata.cache.size: 10%```
 I also would like to check what are the most time consuming calls on ES, but the slowlog thresholds are high
 so let's put:

 ```
 ```index.search.slowlog.threshold.query.warn: 5s
 index.indexing.slowlog.threshold.index.warn: 6s```
 #+END_EXAMPLE
**** index.search.slowlog.threshold.query.warn is index-level setting: https://www.elastic.co/guide/en/elasticsearch/guide/2.x/logging.html
 index.search.slowlog.threshold.query.warn
 index.indexing.slowlog.threshold.index.warn
**** indices.breaker.total.limit
 Changed below:
 ```
 indices.breaker.total.limit: 35%
 ```

 curl -XPUT $es_ip:9200/_cluster/settings -d '
 {
     "persistent" : {
         "indices.breaker.total.limit": "35%"
     }
 }'

 curl $es_ip:9200/_cluster/settings
**** indices.fielddata.cache.size: curl can't change indices.fielddata.cache.size
 indices.fielddata.cache.size: 10%

 curl -XPUT $es_ip:9200/_cluster/settings -d '
 {
     "persistent" : {
         "indices.fielddata.cache.size": "10%"
     }
 }'

 curl $es_ip:9200/_cluster/settings
*** DONE [#A] check es hotthread: curl $es_ip:9200/_nodes/hot_threads
    CLOSED: [2018-02-07 Wed 10:49]
*** #  --8<-------------------------- separator ------------------------>8-- :noexport:
*** DONE [#A] How to speed up ES rebalancing: node_concurrent_recoveries
    CLOSED: [2018-02-10 Sat 23:57]
 #+BEGIN_SRC sh
 curl -XPUT "http://$es_ip:9200/_cluster/settings" -d '{
  "transient" : {
  "cluster.routing.allocation.cluster_concurrent_rebalance": 7,
  "cluster.routing.allocation.node_initial_primaries_recoveries": 7,
  "cluster.routing.allocation.node_concurrent_recoveries": 7,
  "cluster.routing.allocation.enable": "all"
  }
 }'
 #+END_SRC

 curl -XGET http://$es_ip:9200/_cluster/settings

 - revert back
 #+BEGIN_SRC sh
 curl -XPUT "http://$es_ip:9200/_cluster/settings" -d '{
  "transient" : {
  "cluster.routing.allocation.cluster_concurrent_rebalance": 3,
  "cluster.routing.allocation.node_initial_primaries_recoveries": 3,
  "cluster.routing.allocation.node_concurrent_recoveries": 3,
  "cluster.routing.allocation.enable": "all"
  }
 }'
 #+END_SRC
*** DONE elasticsearch get node setting about ES circule breaker
    CLOSED: [2018-02-25 Sun 09:37]
 curl -XGET "http://$es_ip:9200/_nodes/138.68.8.229/_all?pretty"
*** DONE elasticsearch change index replica count
    CLOSED: [2018-05-17 Thu 20:51]
 for index in $(curl $es_ip:9200/_cat/indices?v | grep "open" | awk -F" " '{print $3}'); do
      echo "curl -XPUT http://$es_ip:9200/$index/_settings"
      curl -XPUT http://$es_ip:9200/$index/_settings -d '
      {
      "index" : {
         "number_of_replicas" : 2
         }
      }'
 done


* org-mode configuration                                           :noexport:
#+STARTUP: overview customtime noalign logdone showall
#+DESCRIPTION: 
#+KEYWORDS: 
#+AUTHOR: Denny Zhang
#+EMAIL:  denny@dennyzhang.com
#+TAGS: noexport(n)
#+PRIORITIES: A D C
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_EXCLUDE_TAGS: exclude noexport
#+SEQ_TODO: TODO HALF ASSIGN | DONE BYPASS DELEGATE CANCELED DEFERRED
#+LINK_UP:   
#+LINK_HOME: 
* TODO upgrade from elasticsearch2 to elasticsearch 5              :noexport:
https://www.linkedin.com/pulse/upgrading-elasticsearch-2x-5x-gian-giovani/
http://www.idata.co.il/2016/11/upgrading-from-elasticsearch-2-x-to-5-0/
https://www.metabrik.org/blog/2016/11/19/upgrading-an-elasticsearch-cluster-from-2-x-to-5-0-0/
