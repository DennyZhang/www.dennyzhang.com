* 4 Challenges In Kubernetes Log Transport                  :BLOG:Kubernetes:
:PROPERTIES:
:type:     Kubernetes, Logging, PKS
:END:
---------------------------------------------------------------------
For the past three months, I have been working on [[https://pivotal.io/platform/pivotal-container-service][PKS]] observability features. Right now, it's mostly about kubernetes logging feature.

hmm, logging? Collect logs, and send them to log endpoint. That looks quite straightforward. Simple and Common, isn't it? Agree, but only partially. I have noticed some new challenges in the container logging, compared to VM or bare metal envs.

Here are the summary. Check it out! See how much it may apply to your kubernetes projects. (BTW, our PKS project is [[https://vmware.rolepoint.com/?shorturl=qeEMe][hiring]])

[[image-blog:5 Challenges In Kubernetes Log Transport][https://cdn.dennyzhang.com/images/blog/www/fluentd.png]]
---------------------------------------------------------------------
TODO: Anit-goal of this post

Normally log transport workflow is like below:

TODO: add picture

So you might think problem has been solved. *Not Yet, My Friends*.

[[color:#c7254e][Running Service In Containers Are Different From VMs Or Bare Metal]].
1. Process is more ephemeral
- Deployment is more distributed

Consequently log collecting in container world is different.
---------------------------------------------------------------------
*Challenge I: Fail To Collect All Critical Logs*

- _Pods may get recreated within several seconds_. This means the old log transport mechanism may fail to collect logs for those short-lived pods. But your users may need them for trouble shooting!

When something is wrong, pods may get deleted or recreated quickly. Consequently the log file associated with that pod/container will be deleted/created quickly. 

However log agent like [[https://www.fluentd.org/][fluentd]] or [[https://www.elastic.co/products/logstash][logstash]] detects new log file by scanning the folder or log pattern periodically. And the default scan interval is 60 seconds(see below figure). The scan interval may be too slow to capture the short-lived pods. Yes, you can set the interval to shorter, say 1 second. But you do know the drawbacks, right?

[[4 Challenges In Kubernetes Log Transport][https://raw.githubusercontent.com/dennyzhang/www.dennyzhang.com/master/kubernetes/kubernetes-logging/fluentd-scan-interval.png]]

Previously this won't be a problem in VM world. When process gets restarted somehow, log file may be rotated but won't be deleted. So users may experience slowness for receiving logs. But not like this: *Missing critical log for problematic processes*.

How we can solve this? Not sure about the best practice, since here in PKS we are also exploring. Maybe we can start a kubernetes controller subscribing to pod events. Whenever a pod creation event has been fired, notify log agent immediately. [[https://github.com/honeycombio/honeycomb-kubernetes-agent][honeycomb-kubernetes-agent]] is an interesting GitHub repo implementing this idea. Please leave us comments, if you have a better solution.

- _Not all logs are redirected to stdout/stderr_. If process inside pod writes log to local file instead of stdout/stderr, log agent won't get it.

Why? It only monitors the log file associated with the pod, like below. And that log file will only capture container's stdout/stderr.

#+BEGIN_EXAMPLE
# ls -1  /var/lib/docker/containers/*/*-json.log
ls -1  /var/lib/docker/containers/*/*-json.log
/var/lib/docker/containers/0470.../0470...-json.log
/var/lib/docker/containers/0645.../0645...-json.log
/var/lib/docker/containers/12d2.../12d2...-json.log
...
...
#+END_EXAMPLE

Yes, this logging behavior is *anit-pattern* for kubernetes world. However cloud-native movement definitely takes time, not everyone is fashion enough. This is especially true for DB services.

Compared to VM worlds, Pod may move across different worker nodes quite often. But you don't want whenever k8s cluster has one pod change, the log agent needs to be reloaded or restarted. New challenges, right?

How to solve it? TODO: sidecar, log mapping
---------------------------------------------------------------------
*Challenge II: Multi-tenancy For Namespace Logging*
- _Different log endpoint for different namespace_

Kubernetes workloads are usually running in shared worker VMs. Workloads from different projects are divided by namespaces.

*Different projects may have been different preferences for logging*. Where the log goes to, and managed by what tools, etc. Need to provide an easy way to configure and with no extra security compromises.

It turns out kubernetes [[https://kubernetes.io/docs/reference/glossary/?all=true#term-CustomResourceDefinition][CRD (CustomResourceDefinition)]] is a good fit. 
1. All you need to learn is the standard *kubectl command*. (See [[https://cheatsheet.dennyzhang.com/cheatsheet-kubernetes-a4][kubectl cheatsheet]]). 
- RBAC can be applied to this custom resource. So security can be easily enforced.

In PKS, we call this feature as [[https://docs.pivotal.io/runtimes/pks/1-2/create-sinks.html#define-resource][sink resource]]. Note: this idea has been proposed to kubernetes community. Hopeful it will be merged into upstream soon.

[[4 Challenges In Kubernetes Log Transport][https://raw.githubusercontent.com/dennyzhang/www.dennyzhang.com/master/kubernetes/kubernetes-logging/pks-sink-resource.png]]
---------------------------------------------------------------------
*Challenge III: Support Logging SLA For Different Namespaces*
- _Single instance of log agent per worker node_

For simplicity, people usually only deploy one log agent as kubernetes daemonset. It means one pod per kubernetes worker node. If somehow this pod needs to be reloaded or rescheduled, it will impact all Pods living in this worker node.

Starting from k8s v1.12, each node may run [[https://kubernetes.io/docs/setup/cluster-large/][100 pods]]. Need to make sure your log agent is fast enough to collect logs from all the pods.

[[4 Challenges In Kubernetes Log Transport][https://raw.githubusercontent.com/dennyzhang/www.dennyzhang.com/master/kubernetes/kubernetes-logging/k8s-large.png]]

Like any shared env, you may experience *noisy neighborhood issue*. The misbehaviors of one Pod will penalty all other pods in the same worker node. Want to disable logging for one problematic namespace? You can easily avoid emitting the log, but not the part of collecting log.

- _No gurantee for log transport latency_. [[https://en.wikipedia.org/wiki/Back_pressure][Back-pressure]] should be properly handled. Slow disk create significant latencies for log agent.
---------------------------------------------------------------------
*Challenge IV: Handle Logging From Different Layers*

TODO: 
- k8s addon, control panel, infra level
- Security threats from layered log visualizations
---------------------------------------------------------------------
What is PKS? [[https://pivotal.io/platform/pivotal-container-service][PKS]] is an enterprise Kubernetes solution from VMware and Pivotal.

[[color:#c7254e][Interested in PKS job opportunities?]] Here is [[https://vmware.rolepoint.com/?shorturl=qeEMe][my referral link]].

Open the link, then search with "PKS". You will see all openings. Ping me directly, if you're applying Palo Alto positions specifically.

[[4 Challenges In Kubernetes Log Transport][https://cdn.dennyzhang.com/images/blog/work/vmware_pks.png]]

More Reading: [[https://cheatsheet.dennyzhang.com/cheatsheet-kubernetes-a4][kubectl cheatsheet]], [[https://cheatsheet.dennyzhang.com/cheatsheet-kubernetes-yaml][kubernetes yaml templates]]

#+BEGIN_HTML
<a href="https://github.com/dennyzhang/www.dennyzhang.com/tree/master/kubernetes/kubernetes-logging"><img align="right" width="200" height="183" src="https://www.dennyzhang.com/wp-content/uploads/denny/watermark/github.png" /></a>

<div id="the whole thing" style="overflow: hidden;">
<div style="float: left; padding: 5px"> <a href="https://www.linkedin.com/in/dennyzhang001"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/linkedin.png" alt="linkedin" /></a></div>
<div style="float: left; padding: 5px"><a href="https://github.com/dennyzhang"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/github.png" alt="github" /></a></div>
<div style="float: left; padding: 5px"><a href="https://www.dennyzhang.com/slack" target="_blank" rel="nofollow"><img src="https://slack.dennyzhang.com/badge.svg" alt="slack"/></a></div>
</div>

<br/><br/>
<a href="http://makeapullrequest.com" target="_blank" rel="nofollow"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" alt="PRs Welcome"/></a>
#+END_HTML

Blog URL: https://www.dennyzhang.com/kubernetes-logging
** Skip pods per namespace                                         :noexport:
Denny Zhang [19 hours ago]
@XXX, fluent-bit will still scan logs from pods of "disabled" namespaces. Just fb filter will drop the messages.

So if that namespace keeps logging crazily, the expected performance improvement from disabling that namespace log draining won't happen.

Right? (edited)


XXX [3 hours ago]
Ah, I see what you are talking about now.


XXX [3 hours ago]
This would be something we need to measure to see how bad a performance impact it is. We may move away from hitting disk entirely in the future so I'd hate to invest a lot of time into mitigating this. Do you mind creating a story in the icebox and let XXX know so he is aware?


Denny Zhang [1 hour ago]
Sure. Will do

Yeah, I start this conversation mostly for discussions.  Not intentions to change anything at current stage


XXX [1 hour ago]
We could limit the `[INPUT]` to only the files for containers in our namespace. But that might be a bit involved. Controller would have to do more work and roll the daemonset more often when containers get created or destroyed in the monitored 

- High latency of log collecting
** basic use                                                       :noexport:
In this presentation, we will share our learnings about
enterprise logging for microservices architecture. We will highlight
key reliability and security features that large enterprise dev teams
require when implementing microservices architectures. We will discuss
the current state of microservices logging, the new challenges it
poses for large enterprise dev teams and then we will follow up with
suggestions on how to address these challenges with a quick demo in
the end.
* TODO Questions                                                   :noexport:
** Secure infra level logging
** TODO syslog endpoint protection
** TODO get all pods
* org-mode configuration                                           :noexport:
#+STARTUP: overview customtime noalign logdone showall
#+DESCRIPTION: 
#+KEYWORDS: 
#+AUTHOR: Denny Zhang
#+EMAIL:  denny@dennyzhang.com
#+TAGS: noexport(n)
#+PRIORITIES: A D C
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_EXCLUDE_TAGS: exclude noexport
#+SEQ_TODO: TODO HALF ASSIGN | DONE BYPASS DELEGATE CANCELED DEFERRED
#+LINK_UP:   
#+LINK_HOME: 
